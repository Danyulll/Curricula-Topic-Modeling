---
title: "Curricula-Topic-Modeling"
---

## Introduction to Topic Modeling and LDA

### Terminology

Before beginning there are three important preliminary definitions one will need in any text analysis:

-   Document: a distinct text object that one wishes to analyze. This could be a paper, a paragraph, etc.
-   Term: an individual word in a given document.
-   Corpus: The set of all documents.

### Topic Models

Topic Modeling is a statistical model that attempts to cluster the words found in a document into various "topics". The hope is that these topics would capture some underlying subject contained within the text. It is important to note that Topic Models are fuzzy in the sense that documents are assumed to be comprised of multiple topics with varying degrees of membership to each.

### Latent Dirichlet Allocation

Perhaps the most famous topic model is *Latent Dirichlet Allocation* (LDA) (Blei et al., 2003). It is a three-level hierarchical Bayesian model that defines a collection of documents as a random mixture over some underlying topic set where each topic is itself a distribution over our vocabulary.

LDA works by assuming a data generating process (DGP) for our documents and then employs inference to discover the most likely parameters (Grimmer et al., 2022).

Blei, Ng, and Jordan define the following DGP (2003):

1.  Choose $N \sim$ Poisson($\xi$)
2.  Choose $\theta \sim$ Dir($\alpha$)
3.  For each of the $N$ words $w_n$:
    1.  Choose a topic $z_n\sim$Multinomial($\theta$)
    2.  Choose a word $w_n$ from $p(w_n|z_n,\beta)$, a multinomial probability conditioned on the topic $z_n$

For the following analysis we estimate the parameters using the *collapsed Gibbs sampling method*, though it is important to note there are others that yield varied results. To further complicate things, when fitting LDA in R one must predefine the number of topics for the model. Finding a good estimate for the number of topics is paramount and many methods are explored.

Once a model is fitted, it is common to extract 2 matrices $\Phi$ and $\Theta$. These are also commonly referred to as $\beta$ and $\gamma$ respectively. Both are given by the priors of LDA. $\Phi$ represents the probability mass over the terms. $\Theta$ represents the probability mass function over the topics or the per-document-per-topic probabilities.


## Feature Extraction and Preprocessing

### Document-Term Matrices

There are many methods of feature extraction from text, we opt for the *bag of words* model. To construct this model simply define a common set of words shared between documents and store a count of word appearances for each document. Commonly this is stored as a *Document-Term Matrix* (DTM), for example, given the documents:

::: {#tbl-panel layout-ncol="2"}
| Document 1   |
|--------------|
| "I am happy" |

| Document 2 |
|------------|
| "I am sad" |

Corpus
:::

the corresponding DTM would be

|           | I   |  am | happy | sad |
|-----------|:----|----:|:-----:|:---:|
| **Doc 1** | 1   |   1 |   1   |  0  |
| **Doc 2** | 1   |   1 |   0   |  1  |

In R we can us the tm package to create our Corpus and DTM objects. For these examples scrapped Data Science course information, from various universities, will be preprocessed.

First this data will be examined from a degree pathway perspective with the documents being concatenated course descriptions for entire curricula. These curricula are constructed for each university by sampling courses from each university's Data Science course calendar.

```{r Libraries, echo=FALSE}
# Load Libraries
library(tm)
library(topicmodels)
library(ggplot2)
library(ldatuning)
library(LDAvis)
library(dplyr)
library(stringi)
library(tidytext)
library(tidyr)
library(wordcloud)
library(graphics)
library(slam)
```


```{r Load in Data}
# Loading in data
load("../data/RObjects/degree_corpus.RData")

# Examining which universities are represented 
print(degree_corpus$doc_id)
```

```{r Constructing Corpus}
ds <- DataframeSource(degree_corpus)
corpus <- Corpus(ds)
inspect(corpus[1])
```

As shown the first element of the corpus is from Western University and contains all course description terms from a given pathway through their Data Science calendar.

### Complexity Reduction

Raw text data lends itself to difficult analysis. NLP posses several best practices for cleaning text:

-   Punctation/Number Removal: deleting any non alphabetic characters.
-   Lowercasing: sending all words to lowercase.
-   Stopword Removal: stopwords are words used often in sentences that give little to no information, i.e., atricles such as the, a, etc.
-   Stemming: truncating the ends of words so that they share a common base, i.e., fishing and fishes would be transformed ot fish.
-   Tokenizing: dividing a document into a set of individual words

```{r Text Preprocessing}
# Remove punctuation
corpus <- tm_map(corpus, content_transformer(removePunctuation))
# Send everything to lower case
corpus <- tm_map(corpus, content_transformer(tolower))
# Remove stopwords
corpus <- tm_map(corpus, content_transformer(removeWords), 
          stopwords("english"))
# Remove whitespace
corpus <- tm_map(corpus, stripWhitespace)
# Remove numbers
corpus <- tm_map(corpus, content_transformer(removeNumbers))
# Remove custom stop words
corpus <- tm_map(corpus, function(x) {removeWords(x,c("data","will","students","fall","spring","important","one","considered","stacks","offers","types","may","held","former","honours","faculty","related","enter","review","enrolment","exercises","summer","need","offered"))})
# Stemming
corpus <- tm_map(corpus, stemDocument)

inspect(corpus[1])
```

Now we may construct our DTM,

```{r DTM Construction}
# Create DTM object
dtm <- DocumentTermMatrix(corpus)
inspect(dtm[,1:9])
```

### TF-IDf

If one wishes to further reduce complexity there exist various heuristics for term inclusion. One such heuristic is *term frequency inverse document frequency* (TF-IDF). The intuition for TF-IDF is that extremely frequent words hold language together but do not provide great insight of topic or context. Meanwhile rare words contain a plethora of information but lack the frequency needed to be used in generalizations. Therefore words somewhere in the middle of these extremes ought to be included (Grimmer et al., 2022).

The quantity itself is the multiplication of the *term frequency* (TF) and *inverse document drequency* (IDF). TF is simply the number of times the term $j$ appears in a given document. IDF is defined as

$$idf(term)=ln\left(\frac{{\text{\# of doucments}}}{{\text{\# documents containing term j}}}\right)$$

IDF is a penalty term that adjusts term frequency based on the term's rarity (Silge & Robinson, 2017).

```{r TF-IDF weighting 1}
tfidf_dtm <- weightTfIdf(dtm)
inspect(tfidf_dtm[,1:5])
```

Now our DTM contains TF-IDF scores as the entries as opposed to just TF. We can now filter away terms below a certain threshold of TF-IDF. A common approach is to filter terms away below the median.

```{r TF-IDF Median Calc}
tfidf_mat <- as.matrix(tfidf_dtm)
median(tfidf_mat[tfidf_mat > 0])
```

```{r Filter TF-IDF below median}
tfidf_dtm_r <- tfidf_dtm[,tfidf_dtm$v > 0.0022]
```

```{r TF-IDF DTM Dimensionality}
# Dimensions before reduction
dim(tfidf_dtm)

# Dimensions after reduction
dim(tfidf_dtm_r)
```

With this our dimensionality has sufficiently reduced. It is important to note when we fit a LDA model the topicmodels package in R requires us to have term frequencies as our DTM entries so we must convert back to a TF weighting.

```{r Convert back to TF weighting}
# Get our list of tfidf terms       
terms <- tfidf_dtm_r$dimnames$Terms

# Filter our original dtm
dtm <- dtm[,which(dtm$dimnames$Terms %in% terms)]

# Check to make sure we filtered the correct terms
sum(dtm$dimnames$Terms == terms) == 702

```

## Exploring Degree Pathways

### Topic Number Discovery

LDA, like many unsupervised learning methods, is extremely dependent on its parameters. One such parameter that requires meticulous tuning is the number of topics. In the literature there exist many metrics for discovering the optimal number of topics with several being readily available in R.

#### Perplexity

Perplexity is a measure of a model's ability to generalize to unseen data. It is the log-likelihood of a a test set of data and is given by PleplÃ©:

$$\ell(w)=\text{log}p(w|\Phi,\alpha)=\sum_d \text{log} p(w_d|\Phi,\alpha)$$

- $w_d$: the unseen document
- $\Phi$: the matrix for the topics
- $\alpha$: the hyperparameter for the topic distributions

The lower our perplexity score the better. We can use 3-fold cross validation and plot our perplexities for various topic numbers.

```{r Perplexity 3-fold CV, cache=TRUE}
set.seed(87460945)

# Calculate folds
idxs <- sample(seq_len(9))
folds <- split(idxs, rep(1:3, each = 3, length.out = 9))

# Define number of topics
topics <- seq(2, 50, 1)

# Create data frame for storing results
results <- data.frame()

# Perform cross validation
for (k in topics) {
  scores <- c()
  for (i in 1:3) {
    test_idx <- folds[[i]]
    train_idx <- setdiff(unlist(folds, use.names = FALSE), test_idx)

    test <- dtm[test_idx, ]
    train <- dtm[train_idx, ]

    LDA.out <- LDA(dtm, k, method = "Gibbs")
    p <- perplexity(LDA.out, newdata = test)
    scores <- c(scores, p)
  }
  temp <- data.frame("K" = k, "Perplexity" = mean(scores))
  results <- rbind(results, temp)
}

# Plot Perplexity vs. K
ggplot(results, aes(x=K, y=Perplexity)) + geom_line() + ggtitle("Perplexity vs. Number of Topics K")+
  theme(plot.title = element_text(hjust = 0.5))
```

```{r min Perplexity}
# Retrieving K for minimum Perplexity
results[which.min(results$Perplexity),"K"]
```

Looks like in terms of Perplexity 8 is our optimal number of topics.

#### Coherence

**TO DO Do this when I have time**

#### ldatuning

The ldatuning package provides us with several metrics to evaluate the number topics all of which rely on find extrema. We seek to minimize Arun2010 and CaoJuan2009 and maximize Deveaud2014 and Griffiths2004.

```{r ldatuning k 40,cache=TRUE}
# Find number of topics
result <- FindTopicsNumber(
  dtm,
  topics = seq(from = 2, to = 40, by = 1),
  metrics = c("Griffiths2004", "CaoJuan2009", "Arun2010", "Deveaud2014"),
  method = "Gibbs",
  control = list(seed = 77),
  mc.cores = 2L,
)
# Plot metrics
suppressWarnings(FindTopicsNumber_plot(result))
```

It seems no topic numbers over 21 are viable. Let us zoom in on the 2-21 range.

```{r ldatuning k 21,cache=TRUE}
# Find number of topics
result <- FindTopicsNumber(
  dtm,
  topics = seq(from = 2, to = 21, by = 1),
  metrics = c("Griffiths2004", "CaoJuan2009", "Arun2010", "Deveaud2014"),
  method = "Gibbs",
  control = list(seed = 77),
  mc.cores = 2L,
)
# Plot metrics
suppressWarnings(FindTopicsNumber_plot(result))
```

For our minima we see CaoJuan2009 suggests 6 and 9 while Arun2019 promots 19. Examining our maxima we find Deveaud2014 is uninterpretable and Griffiths2004 recommends 10 and 20. This provides us with a good range of topic numbers to explore, $k = 6,8,9,10,19,20$.

#### Visualizing topics with LDAvis

Rather than individually fit each model model for our topic numbers the R package LDAvis creates a fantastic interactable visualization of our LDA model. LDAvis tries to answer 3 questions (Sievert & Shirley, 2014): 

- How prevalent is each topic? 
- How do the topics relate to each other? 
- How do the topics relate to each other?

```{r LDAvis, cache=TRUE}
# Fit largest model
lda.out <- LDA(dtm, 19, method = "Gibbs")

# Find required quantities
phi <- posterior(lda.out)$terms %>% as.matrix
theta <- posterior(lda.out)$topics %>% as.matrix
vocab <- colnames(phi)
doc_length <- vector()
for (i in 1:length(corpus)) {
  temp <- paste(corpus[[i]]$content, collapse = ' ')
  doc_length <-
    c(doc_length, stri_count(temp, regex = '\\S+'))
}
temp_frequency <- as.matrix(dtm)
freq_matrix <- data.frame(ST = colnames(temp_frequency),
                          Freq = colSums(temp_frequency))

# Convert to json
json_lda <- LDAvis::createJSON(
  phi = phi,
  theta = theta,
  vocab = vocab,
  doc.length = doc_length,
  term.frequency = freq_matrix$Freq
)

# Open server for visualization
serVis(json_lda, out.dir = "../LDAvis", open.browser = FALSE)
```


  <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
  
  <script src="./LDAvis/d3.v3.js"></script>
  <script src="./LDAvis/ldavis.js"></script>
  <link rel="stylesheet" type="text/css" href="./LDAvis/lda.css">



  <div id = "lda"></div>
  <script>
    var vis = new LDAvis("#lda", "./LDAvis/lda.json");
  </script>

To interpret this visualization we fist need to define several quantities:

*Relevance* - Sievert and Shirley propose relevance  as a metric to rank terms based on their usefulness for interpreting topics (2014). They define it as follows:

$$r(w,k|\lambda)=\lambda \text{log}(\phi_{kw}+(1-\lambda)\text{log}\left( \frac{\phi_{kw}}{p_w}\right)$$

- $\phi_{kw}$: the probability of term $w$ for topic $k$.
- $p_w$: the marginal probability of term $w$ in the corpus.
- $\lambda$: a weight parameter such that $0 \leq \lambda \leq 1$.
- $\text{log}\left( \frac{\phi_{kw}}{p_w}\right)$: the lift of a term.

A $\lambda$ of 1 orders terms based on their topic-specific probability whereas a $\lambda$ of 0 orders term based on their lift. Sievert and Shirley suggest $\lambda = 6$. The higher this metric the better.

*Lift* - As seen above Lift is the ratio of a term's probability in a given topic to its marginal probability. The metric downweights terms that are frequent throughout an entire corpus (Sievert & Shirley 2014).

*Saliency* - Introuced by Chuang, Manning, and Heer, it is defined as follows (2012):
$$\begin{align} saliency(w) &= P(W) \times distinctiveness(w) \\
distinctiveness(w) &= \sum_T P(T|w)\text{log}\frac{P(T|w)}{P(T)}
\end{align}$$

- $P(T|w)$: the probability a word $w$ was genereted by a topic $T$
- $P(T)$: the marginal probability of the topic, i.e., the probability a word was produced b topic $T$

The general idea is that distinctiveness is the Kullback-Leibler divergence between $P(T|w)$ and $P(T)$ and informs us how useful a $w$ is for determining a generating topic. Multiplying this by $P(w)$ yields saliency; a metric that ranks terms in their usefulness in identifying topics.

With these we can understand our visuals:

- Topic circles: circles are drawn equal to the number of topics. Their areas are proportional to the estimated number of words produced by that topic. The centers of the circles are determined by computing the distances between topics which are then projected onto a 2D plane. The larger a circle is, the more prevalent it is.

-   Red Bars: the estimated number of times a term was created by a  topic. When you select a circle (topic) the red bars for the most relevant terms will be highlighted.

-   Blue Bars: each bar represents the frequency of each term. When no topic is selected the blue bars for the most salient terms are displayed. When a topic is selected the blue bars are the frequency of the most relevant terms.

-   Topic-Term Circles: When you highlight a term the circles' areas will change. These circles' areas are proportional to the number of times a topic generated that specific term.

If $\lambda=1$ terms are ranked in decreasing order of their topic-specific probability. $\lambda=0$ ranks terms solely by their lift. The optimal $\lambda$ is $\lambda=0.6$.

### Inspecting $\beta$

Given our optimal model we may now move onto exploring what words comprise each of the topics. To do so we first plot the terms with the highest $\beta$ values and then move onto wordclouds. Recall $\beta$ provides us with the per-topic-per-word probabilities. These are the probability a term was created by a topic.

```{r Graph Beta University Level}
#| fig-width: 10
#| fig-height: 10
uni_topics <- tidy(lda.out,matrix="beta")

top_terms <- uni_topics %>%
 group_by(topic) %>%
 top_n(9, beta) %>%
 ungroup() %>%
 arrange(topic, -beta) %>%
  mutate(term = reorder(term, beta))

top_terms %>%
 ggplot(aes(term, beta, fill = factor(topic))) +
 geom_col(show.legend = FALSE) +
 facet_wrap(~ topic, scales = "free") +
 coord_flip()
```

Examining word clouds may also prove fruitful.

```{r wordcloud beta topics in university}

set.seed(87460945)

top_terms <- uni_topics %>%
 group_by(topic) %>%
 top_n(20, beta) %>%
 ungroup() %>%
 arrange(topic, -beta) %>%
  mutate(term = reorder(term, beta))

# for (i in 1:20) {
# df <- subset(as.data.frame(top_terms),topic==i)  
# wordcloud(df$term, freq = df$beta,colors= c("indianred1","indianred2","indianred3","indianred"))
# title(paste("Topic ",i), font.main = 1)
# }
```
::: {.panel-tabset}

# Topic 1
```{r, echo=FALSE}
df <- subset(as.data.frame(top_terms),topic==1)
suppressWarnings(wordcloud(df$term, freq = df$beta,colors= c("indianred1","indianred2","indianred3","indianred"))) 

```

# Topic 2
```{r, echo=FALSE}
df <- subset(as.data.frame(top_terms),topic==2)
suppressWarnings(wordcloud(df$term, freq = df$beta,colors= c("indianred1","indianred2","indianred3","indianred")))
```

# Topic 3
```{r, echo=FALSE}
df <- subset(as.data.frame(top_terms),topic==3)
suppressWarnings(wordcloud(df$term, freq = df$beta,colors= c("indianred1","indianred2","indianred3","indianred")))
```

# Topic 4
```{r, echo=FALSE}
df <- subset(as.data.frame(top_terms),topic==4)
suppressWarnings(wordcloud(df$term, freq = df$beta,colors= c("indianred1","indianred2","indianred3","indianred")))
```

# Topic 5
```{r, echo=FALSE}
df <- subset(as.data.frame(top_terms),topic==5)
suppressWarnings(wordcloud(df$term, freq = df$beta,colors= c("indianred1","indianred2","indianred3","indianred")))
```

# Topic 6
```{r, echo=FALSE}
df <- subset(as.data.frame(top_terms),topic==6)
suppressWarnings(wordcloud(df$term, freq = df$beta,colors= c("indianred1","indianred2","indianred3","indianred")))
```

# Topic 7
```{r, echo=FALSE}
df <- subset(as.data.frame(top_terms),topic==7)
suppressWarnings(wordcloud(df$term, freq = df$beta,colors= c("indianred1","indianred2","indianred3","indianred")))
```

# Topic 8
```{r, echo=FALSE}
df <- subset(as.data.frame(top_terms),topic==8)
suppressWarnings(wordcloud(df$term, freq = df$beta,colors= c("indianred1","indianred2","indianred3","indianred")))
```


# Topic 9
```{r, echo=FALSE}
df <- subset(as.data.frame(top_terms),topic==9)
suppressWarnings(wordcloud(df$term, freq = df$beta,colors= c("indianred1","indianred2","indianred3","indianred")))
```

# Topic 10
```{r, echo=FALSE}
df <- subset(as.data.frame(top_terms),topic==10)
suppressWarnings(wordcloud(df$term, freq = df$beta,colors= c("indianred1","indianred2","indianred3","indianred")))
```


# Topic 11
```{r, echo=FALSE}
df <- subset(as.data.frame(top_terms),topic==11)
suppressWarnings(wordcloud(df$term, freq = df$beta,colors= c("indianred1","indianred2","indianred3","indianred"))) 

```

# Topic 12
```{r, echo=FALSE}
df <- subset(as.data.frame(top_terms),topic==12)
suppressWarnings(wordcloud(df$term, freq = df$beta,colors= c("indianred1","indianred2","indianred3","indianred")))
```

# Topic 13
```{r, echo=FALSE}
df <- subset(as.data.frame(top_terms),topic==13)
suppressWarnings(wordcloud(df$term, freq = df$beta,colors= c("indianred1","indianred2","indianred3","indianred")))
```

# Topic 14
```{r, echo=FALSE}
df <- subset(as.data.frame(top_terms),topic==14)
suppressWarnings(wordcloud(df$term, freq = df$beta,colors= c("indianred1","indianred2","indianred3","indianred")))
```

# Topic 15
```{r, echo=FALSE}
df <- subset(as.data.frame(top_terms),topic==15)
suppressWarnings(wordcloud(df$term, freq = df$beta,colors= c("indianred1","indianred2","indianred3","indianred")))
```

# Topic 16
```{r, echo=FALSE}
df <- subset(as.data.frame(top_terms),topic==16)
suppressWarnings(wordcloud(df$term, freq = df$beta,colors= c("indianred1","indianred2","indianred3","indianred")))
```

# Topic 17
```{r, echo=FALSE}
df <- subset(as.data.frame(top_terms),topic==17)
suppressWarnings(wordcloud(df$term, freq = df$beta,colors= c("indianred1","indianred2","indianred3","indianred")))
```

# Topic 18
```{r, echo=FALSE}
df <- subset(as.data.frame(top_terms),topic==18)
suppressWarnings(wordcloud(df$term, freq = df$beta,colors= c("indianred1","indianred2","indianred3","indianred")))
```


# Topic 19
```{r, echo=FALSE}
df <- subset(as.data.frame(top_terms),topic==19)
suppressWarnings(wordcloud(df$term, freq = df$beta,colors= c("indianred1","indianred2","indianred3","indianred")))
```

# Topic 20
```{r, echo=FALSE}
df <- subset(as.data.frame(top_terms),topic==20)
suppressWarnings(wordcloud(df$term, freq = df$beta,colors= c("indianred1","indianred2","indianred3","indianred")))
```

:::

### Inspecting $\gamma$

Another interesting question to explore is which topics do universities prioritize? Is one university's Data Science program more heavily biased towards Math? How about Statistics? Recall the $\gamma$ matrix provides us with the per-document-per-topic probabilities. That is, it provides us with a percentage break down of which topics generated each document.

```{r Graph Gamma University Level}
#| fig-width: 10
#| fig-height: 10

uni_topics <- tidy(lda.out,matrix="gamma")

top_terms <- uni_topics %>%
 group_by(topic) %>%
 arrange(topic, gamma)

top_terms %>%
 mutate(topic = reorder(topic, gamma)) %>%
 ggplot(aes(topic, gamma, fill = factor(document))) +
 geom_col(show.legend = FALSE) +
 facet_wrap(~ document, scales = "free") +
 coord_flip()
```

## Inspecting Other Solutions

From our 20 group visualization we see significant overlap on the left clusters. This suggests one of our fewer topic solutions may be more apt. Therefore we opt to visualize the 5, 8, and 10 group solutions. Furthermore, it seems in our word clouds stemming has made interpretability more difficult. Therefore we change our DTM to not include stemming; if similar rooted words appear we can always go back and combine them by hand.

```{r Remaking DTM}
# Loading in data
load("../data/RObjects/degree_corpus.RData")

# Construct corpus object
ds <- DataframeSource(degree_corpus)
corpus <- Corpus(ds)

# Text preprocessing omitting stemming

# Remove punctuation
corpus <- tm_map(corpus, content_transformer(removePunctuation))
# Send everything to lower case
corpus <- tm_map(corpus, content_transformer(tolower))
# Remove stopwords
corpus <- tm_map(corpus, content_transformer(removeWords), 
          stopwords("english"))
# Remove whitespace
corpus <- tm_map(corpus, stripWhitespace)
# Remove numbers
corpus <- tm_map(corpus, content_transformer(removeNumbers))
# Remove custom stop words
remove_words <-
  c("include","methods","topics","design","functions","emphasis","language","introduction","languages","performance","experience","course","science","techniques","variables","number","department","tools","fundamental","also","major","modern","issues","used","methods","using","case","architecture","covered","credit","basic","cosc","granted","use","solutions","data","will","students","fall","spring","important","one","considered","stacks","offers","types","may","held","former","honours","faculty","related","enter","review","enrolment","exercises","summer","need","offered","social","digital","terms","real","concepts","understanding","can","including","programs","program","recommended","examples","introduced","large","search","relations","key","etc","reasoning","intended","fws","general","restricted","version","two","comp","well","rich","intended","required","internet","recent","phys","sciences","covers","year","selected","renewal","explored","csch","principles","practice","development","studies","security","provides","advanced","instruction","discussed","processes","death","lower","high","crncr","taken","efficient","includes","core","retrieval","class","within","present","option","interested","together","session","week","new","order","tables","small","suitable","wide","without","good"
  )
corpus <- tm_map(corpus, function(x) {
  removeWords(x, remove_words)
})
# Create DTM
dtm <- DocumentTermMatrix(corpus)
inspect(dtm)
```

```{r TF-IDF weighting 2}
# Reweight and filter DTM based on mean TF-IDF
tfidf_dtm <- weightTfIdf(dtm)
tfidf_mat <- as.matrix(tfidf_dtm)
median(tfidf_mat[tfidf_mat > 0])
tfidf_dtm_r <- tfidf_dtm[,tfidf_dtm$v > 0.0033]

# Get our list of tfidf terms       
terms <- tfidf_dtm_r$dimnames$Terms

# Filter our original dtm
dtm <- dtm[,which(dtm$dimnames$Terms %in% terms)]
inspect(dtm)
```

### 5 Group Solution

```{r 5 groups vis median tf-idf}
# Fit largest model
lda.out <- LDA(dtm, 5, method = "Gibbs")

# Find required quantities
phi <- posterior(lda.out)$terms %>% as.matrix
theta <- posterior(lda.out)$topics %>% as.matrix
vocab <- colnames(phi)
doc_length <- vector()
for (i in 1:length(corpus)) {
  temp <- paste(corpus[[i]]$content, collapse = ' ')
  doc_length <-
    c(doc_length, stri_count(temp, regex = '\\S+'))
}
temp_frequency <- as.matrix(dtm)
freq_matrix <- data.frame(ST = colnames(temp_frequency),
                          Freq = colSums(temp_frequency))

# Convert to json
json_lda <- LDAvis::createJSON(
  phi = phi,
  theta = theta,
  vocab = vocab,
  doc.length = doc_length,
  term.frequency = freq_matrix$Freq
)

# Open server for visualization
# serVis(json_lda, open.browser = TRUE)
```



### 8 Group Solution

```{r 8 groups vis median tf-idf}
# Fit largest model
lda.out <- LDA(dtm, 8, method = "Gibbs")

# Find required quantities
phi <- posterior(lda.out)$terms %>% as.matrix
theta <- posterior(lda.out)$topics %>% as.matrix
vocab <- colnames(phi)
doc_length <- vector()
for (i in 1:length(corpus)) {
  temp <- paste(corpus[[i]]$content, collapse = ' ')
  doc_length <-
    c(doc_length, stri_count(temp, regex = '\\S+'))
}
temp_frequency <- as.matrix(dtm)
freq_matrix <- data.frame(ST = colnames(temp_frequency),
                          Freq = colSums(temp_frequency))

# Convert to json
json_lda <- LDAvis::createJSON(
  phi = phi,
  theta = theta,
  vocab = vocab,
  doc.length = doc_length,
  term.frequency = freq_matrix$Freq
)

# Open server for visualization
# serVis(json_lda, open.browser = TRUE)
```



### 10 Group Solution

```{r 10 groups vis median tf-idf}
# Fit largest model
lda.out <- LDA(dtm, 10, method = "Gibbs")

# Find required quantities
phi <- posterior(lda.out)$terms %>% as.matrix
theta <- posterior(lda.out)$topics %>% as.matrix
vocab <- colnames(phi)
doc_length <- vector()
for (i in 1:length(corpus)) {
  temp <- paste(corpus[[i]]$content, collapse = ' ')
  doc_length <-
    c(doc_length, stri_count(temp, regex = '\\S+'))
}
temp_frequency <- as.matrix(dtm)
freq_matrix <- data.frame(ST = colnames(temp_frequency),
                          Freq = colSums(temp_frequency))

# Convert to json
json_lda <- LDAvis::createJSON(
  phi = phi,
  theta = theta,
  vocab = vocab,
  doc.length = doc_length,
  term.frequency = freq_matrix$Freq
)

# Open server for visualization
# serVis(json_lda, open.browser = TRUE)
```


## Work in progress
Going to explore cutting TF-IDF at the 25th percentile and no TF-IDF filtering for the 5, 8, and 10 group solutions

### No TF-IDF

```{r Setting up unfiltered dtm}
# Loading in data
load("../data/RObjects/degree_corpus.RData")

# Create corpus object
ds <- DataframeSource(degree_corpus)
corpus <- Corpus(ds)

# Remove punctuation
corpus <- tm_map(corpus, content_transformer(removePunctuation))
# Send everything to lower case
corpus <- tm_map(corpus, content_transformer(tolower))
# Remove stopwords
corpus <- tm_map(corpus,
                 content_transformer(removeWords),
                 stopwords("english"))
# Remove whitespace
corpus <- tm_map(corpus, stripWhitespace)
# Remove numbers
corpus <- tm_map(corpus, content_transformer(removeNumbers))
# Remove custom stop words
remove_words <-
  c("include","methods","topics","design","functions","emphasis","language","introduction","languages","performance","experience","course","science","techniques","variables","number","department","tools","fundamental","also","major","modern","issues","used","methods","using","case","architecture","covered","credit","basic","cosc","granted","use","solutions","data","will","students","fall","spring","important","one","considered","stacks","offers","types","may","held","former","honours","faculty","related","enter","review","enrolment","exercises","summer","need","offered","social","digital","terms","real","concepts","understanding","can","including","programs","program","recommended","examples","introduced","large","search","relations","key","etc","reasoning","intended","fws","general","restricted","version","two","comp","well","rich","intended","required","internet","recent","phys","sciences","covers","year","selected","renewal","explored","csch","principles","practice","development","studies","security","provides","advanced","instruction","discussed","processes","death","lower","high","crncr","taken","efficient","includes","core","retrieval","class","within","present","option","interested","together","session","week","new","order","tables","small","suitable","wide","without","good","introduces","assignments","current","thinking","completed","basics","essential","gain","effective","file","three","many","classes","extensive","tasks","work","meaningful","first","creating","elementary","image"
  )
corpus <- tm_map(corpus, function(x) {
  removeWords(x, remove_words)
})

dtm_notfidf <- DocumentTermMatrix(corpus)
```

Not stemming means we will manually have to combine like terms. For example our corpus contains "math", "mathematics", "mathematically", and "mathematical" so we will need to go in and sum their columns in the DTM.

```{r Define combineCols function}
combineCols <- function(words, dtm){
  # Get indices of columns
idx <- which(colnames(dtm) %in% words)

# Convert sparse matrix dtm to normal matrix
dtm <- as.matrix(dtm)

# Combine columns
dtm[,c(idx[1])] <- rowSums(dtm[,c(idx)])

if(length(words) == 2){
dtm <- dtm[,-idx[2]]  
}else{
dtm <- dtm[,-idx[2:length(words)]]  
}
# Convert back to DTM
dtm <- as.DocumentTermMatrix(as.simple_triplet_matrix(dtm),weighting = weightTf)

return(dtm)
}
```

```{r Combining Columns}
dtm_notfidf <-
  combineCols(c("math", "mathematics", "mathematically", "mathematical"),
              dtm_notfidf)

dtm_notfidf <-
  combineCols(c("model", "modeling", "modelling", "models"), dtm_notfidf)

dtm_notfidf <- combineCols(c("algorithm", "algorithms"), dtm_notfidf)

dtm_notfidf <- combineCols(c("learn", "learning"), dtm_notfidf)

dtm_notfidf <- combineCols(c("stat", "statistics","statistical"), dtm_notfidf)

dtm_notfidf <- combineCols(c("proof", "proofs"), dtm_notfidf)

dtm_notfidf <- combineCols(c("fields", "field"), dtm_notfidf)
dtm_notfidf <- combineCols(c("application","applications", "apply"), dtm_notfidf)

dtm_notfidf <- combineCols(c("logical", "logic"), dtm_notfidf)

dtm_notfidf <- combineCols(c("limit", "limits"), dtm_notfidf)

dtm_notfidf <- combineCols(c("distributions", "distribution"), dtm_notfidf)

dtm_notfidf <- combineCols(c("theorem", "theorems"), dtm_notfidf)

```

Note for the following visualization we will have $\lambda=0.6$ as suggested by Sievert and Shirley.

#### 5 Group Solution

```{r 5 Group No TF-IDF Weighting Vis, cache=TRUE}
# Fit largest model
lda.out <- LDA(dtm_notfidf, 5, method = "Gibbs", control = list(seed = 87460945))

# Find required quantities
phi <- posterior(lda.out)$terms %>% as.matrix
theta <- posterior(lda.out)$topics %>% as.matrix
vocab <- colnames(phi)
doc_length <- vector()
for (i in 1:length(corpus)) {
  temp <- paste(corpus[[i]]$content, collapse = ' ')
  doc_length <-
    c(doc_length, stri_count(temp, regex = '\\S+'))
}
temp_frequency <- as.matrix(dtm_notfidf)
freq_matrix <- data.frame(ST = colnames(temp_frequency),
                          Freq = colSums(temp_frequency))

# Convert to json
json_lda <- LDAvis::createJSON(
  phi = phi,
  theta = theta,
  vocab = vocab,
  doc.length = doc_length,
  term.frequency = freq_matrix$Freq
)

# Open server for visualization
suppressWarnings(serVis(json_lda, open.browser = TRUE))
```
Top terms:
- Group 1: model, applications, systems, software, probability, structures (using software to apply models?)
- Group 2: database, algorithms, regression, theorem, proofs, distributions (databases topic?)
- Group 3: mathematical, statistical, calculus, problems, linear, multiple (math and stats topic)
- Group 4: programming, analysis, business, time, computing, estimation, visualization, management, marketing, algorithms (certainly a programming topic, maybe a data analysis,visualization topic with applications in business?)
- Group 5: information, learning, mining, graph, machine, knowledge, text, linear, big (machine learning topic, big could be for big data)

Top terms for $\beta$
  
```{r}
uni_topics <- tidy(lda.out,matrix="beta")

top_terms <- uni_topics %>%
 group_by(topic) %>%
 top_n(10, beta) %>%
 ungroup() %>%
 arrange(topic, -beta) %>%
  mutate(term = reorder(term, beta))

top_terms %>%
 ggplot(aes(term, beta, fill = factor(topic))) +
 geom_col(show.legend = FALSE) +
 facet_wrap(~ topic, scales = "free") +
 coord_flip()
```

#### 8 Group Solution
```{r 8 Group No TF-IDF Weighting Vis}
# Fit largest model
lda.out <- LDA(dtm_notfidf, 8, method = "Gibbs", control = list(seed = 87460945))

# Find required quantities
phi <- posterior(lda.out)$terms %>% as.matrix
theta <- posterior(lda.out)$topics %>% as.matrix
vocab <- colnames(phi)
doc_length <- vector()
for (i in 1:length(corpus)) {
  temp <- paste(corpus[[i]]$content, collapse = ' ')
  doc_length <-
    c(doc_length, stri_count(temp, regex = '\\S+'))
}
temp_frequency <- as.matrix(dtm_notfidf)
freq_matrix <- data.frame(ST = colnames(temp_frequency),
                          Freq = colSums(temp_frequency))

# Convert to json
json_lda <- LDAvis::createJSON(
  phi = phi,
  theta = theta,
  vocab = vocab,
  doc.length = doc_length,
  term.frequency = freq_matrix$Freq
)

# Open server for visualization
 serVis(json_lda, open.browser = TRUE)
```
Top Terms:
- Group 1: model, applications, programming, systems, algorithms, linear, analysis (applications of models in data analysis)
- Group 2: mathematical, statistical, calculus, multiple, problems, variance, random, optimzation, theory (clearly a math/stats topic)
- Group 3: machine, knowledge, text, mining, objectorientated, classification, system, environment (machine learning topic with focus on data mining)
- Group 4: theorem, logic, skills, proofs, study, inference, estiamtors, compelxity (mathematical proof topics or probability theory topic)
- Group 5: differential, queues, sampling, distributions, regression, databases, liklehood, ordinary (the fact that differential occurs with distribtions, likelihood, ordinary, and regression makes me think of a GLM topic)
- Group 6: information, sets, big, practical, graph, control, problem, network (a graph/set theory topic, potential focus on big data and network analysis)
- Group 7: computing, business, analysis, marketing, writing, team, decision, research (data analysis topic in business/marketing)
- Group 8: equations, partial, implementation, derivatives, chains, critical, physical, applied, actuarial, eigenvalues, privacy, generalized (pretty noisey topic maybe something to do with applied math)

Top terms for $\beta$ (notably this one has a few differences like the statistical learning topic for 2)
```{r}
uni_topics <- tidy(lda.out,matrix="beta")

top_terms <- uni_topics %>%
 group_by(topic) %>%
 top_n(15, beta) %>%
 ungroup() %>%
 arrange(topic, -beta) %>%
  mutate(term = reorder(term, beta))

top_terms %>%
 ggplot(aes(term, beta, fill = factor(topic))) +
 geom_col(show.legend = FALSE) +
 facet_wrap(~ topic, scales = "free") +
 coord_flip()
```

#### 10 Group Solution
```{r 10 Group No TF-IDF Weighting Vis}
# Fit largest model
lda.out <- LDA(dtm_notfidf, 10, method = "Gibbs",control = list(seed = 87460945))

# Find required quantities
phi <- posterior(lda.out)$terms %>% as.matrix
theta <- posterior(lda.out)$topics %>% as.matrix
vocab <- colnames(phi)
doc_length <- vector()
for (i in 1:length(corpus)) {
  temp <- paste(corpus[[i]]$content, collapse = ' ')
  doc_length <-
    c(doc_length, stri_count(temp, regex = '\\S+'))
}
temp_frequency <- as.matrix(dtm_notfidf)
freq_matrix <- data.frame(ST = colnames(temp_frequency),
                          Freq = colSums(temp_frequency))

# Convert to json
json_lda <- LDAvis::createJSON(
  phi = phi,
  theta = theta,
  vocab = vocab,
  doc.length = doc_length,
  term.frequency = freq_matrix$Freq
)

# Open server for visualization
 serVis(json_lda, open.browser = TRUE)
 
 
```
Top Terms
- Group 1: model, applications, algorithms, programming, systems, linear, regression (applications of models in data analysis, likely focus on programming them)
- Group 2: mathematical, statistical, multiple, calculus, varainces, derivatives, optimization (math and stat classes though notably math has a higher estiamted teerm frequency)
- Group 3: learning, machine, integrals, text, mining, web, information, practical, patterns (definitely a machine learning topic though since neither classification nor regression show up probably machine learning with respect to feature extraction whether from text, web, etc)
- Group 4: database, likelihood, logic, simulation, distributions, space, prediction, sample (noisey topic, database has the highest estimated frequency so maybe we can take it as a databases topic)
- Group 5: computing, business, marketing, management, analysis, objectoreintated, communication, quantitative (business data analysis with a focus on programming and communicating results)
- Group 6: theorem, proofs, field, theoretical, estimators (mathematical proof topic, estimators makes me think it includes probability theory)
- Group 7: big, operations, transformation, power, engines, trees, partial, vector, variable, algorithmic, diagonalization, construction (a topic on working with big data and data sets, likely covers transforming data and applying linear algebra techniques like diagonalization to datasets)
- Group 8: problems, learning, realworld, study, finite, reading, database, exponential, inference, crossvalidation (applying datascience / machine learning to realworld studies?)
- Group 9: sets, sampling, matrices, simple, privacy, searching, complex, recursive (privacy mixed with sampling makes me think of a data sampling topic or experiment design)
- Group 10: graphs, implementation, presentation, unsupervised, scientific, java, analytics, automata, python, applying, lifecycle, objective, ridge (noisey topic but something to do with scientific programming)

Top terms for $\beta$
```{r}
uni_topics <- tidy(lda.out,matrix="beta")

top_terms <- uni_topics %>%
 group_by(topic) %>%
 top_n(10, beta) %>%
 ungroup() %>%
 arrange(topic, -beta) %>%
  mutate(term = reorder(term, beta))

top_terms %>%
 ggplot(aes(term, beta, fill = factor(topic))) +
 geom_col(show.legend = FALSE) +
 facet_wrap(~ topic, scales = "free") +
 coord_flip()
```

### TF-IDF 25th Percentile

```{r Setting up 25tfidf dtm}
# Loading in data
load("../data/RObjects/degree_corpus.RData")

# Create corpus object
ds <- DataframeSource(degree_corpus)
corpus <- Corpus(ds)

# Remove punctuation
corpus <- tm_map(corpus, content_transformer(removePunctuation))
# Send everything to lower case
corpus <- tm_map(corpus, content_transformer(tolower))
# Remove stopwords
corpus <- tm_map(corpus,
                 content_transformer(removeWords),
                 stopwords("english"))
# Remove whitespace
corpus <- tm_map(corpus, stripWhitespace)
# Remove numbers
corpus <- tm_map(corpus, content_transformer(removeNumbers))
# Remove custom stop words
remove_words <-
  c("include","methods","topics","design","functions","emphasis","language","introduction","languages","performance","experience","course","science","techniques","variables","number","department","tools","fundamental","also","major","modern","issues","used","methods","using","case","architecture","covered","credit","basic","cosc","granted","use","solutions","data","will","students","fall","spring","important","one","considered","stacks","offers","types","may","held","former","honours","faculty","related","enter","review","enrolment","exercises","summer","need","offered","social","digital","terms","real","concepts","understanding","can","including","programs","program","recommended","examples","introduced","large","search","relations","key","etc","reasoning","intended","fws","general","restricted","version","two","comp","well","rich","intended","required","internet","recent","phys","sciences","covers","year","selected","renewal","explored","csch","principles","practice","development","studies","security","provides","advanced","instruction","discussed","processes","death","lower","high","crncr","taken","efficient","includes","core","retrieval","class","within","present","option","interested","together","session","week","new","order","tables","small","suitable","wide","without","good","introduces","assignments","current","thinking","completed","basics","essential","gain","effective","file","three","many","classes","extensive","tasks","work","meaningful","first","creating","various", "majors"
  )
corpus <- tm_map(corpus, function(x) {
  removeWords(x, remove_words)
})

dtm_25tfidf <- DocumentTermMatrix(corpus)

dtm_25tfidf <-
  combineCols(c("math", "mathematics", "mathematically", "mathematical"),
              dtm_25tfidf)

dtm_25tfidf <-
  combineCols(c("model", "modeling", "modelling", "models"), dtm_25tfidf)

dtm_25tfidf <- combineCols(c("algorithm", "algorithms"), dtm_25tfidf)

dtm_25tfidf <- combineCols(c("learn", "learning"), dtm_25tfidf)

dtm_25tfidf <- combineCols(c("stat", "statistics","statistical"), dtm_25tfidf)

dtm_25tfidf <- combineCols(c("proof", "proofs"), dtm_25tfidf)

dtm_25tfidf <- combineCols(c("fields", "field"), dtm_25tfidf)
dtm_25tfidf <- combineCols(c("application","applications", "apply"), dtm_25tfidf)

dtm_25tfidf <- combineCols(c("logical", "logic"), dtm_25tfidf)

dtm_25tfidf <- combineCols(c("limit", "limits"), dtm_25tfidf)

dtm_25tfidf <- combineCols(c("distributions", "distribution"), dtm_25tfidf)
```

```{r TF-IDF weighting for 25th pt 1}
# Reweight and filter DTM based on mean TF-IDF
tfidf_dtm <- weightTfIdf(dtm_25tfidf)
tfidf_mat <- as.matrix(tfidf_dtm)
summary(tfidf_mat[tfidf_mat > 0])
```

```{r TF-IDF weighting for 25th pt 2}
tfidf_dtm_r <- tfidf_dtm[,tfidf_dtm$v > 0.0020719]

# Get our list of tfidf terms       
terms <- tfidf_dtm_r$dimnames$Terms

# Filter our original dtm
dtm_25tfidf <- dtm_25tfidf[,which(dtm_25tfidf$dimnames$Terms %in% terms)]
```

These will also be examined at $\lambda = 0.6$

#### 5 Group Solution

```{r 5 Group TF-IDF Weighting Vis}
# Fit largest model
lda.out <- LDA(dtm_25tfidf, 5, method = "Gibbs",control = list(seed = 87460945))

# Find required quantities
phi <- posterior(lda.out)$terms %>% as.matrix
theta <- posterior(lda.out)$topics %>% as.matrix
vocab <- colnames(phi)
doc_length <- vector()
for (i in 1:length(corpus)) {
  temp <- paste(corpus[[i]]$content, collapse = ' ')
  doc_length <-
    c(doc_length, stri_count(temp, regex = '\\S+'))
}
temp_frequency <- as.matrix(dtm_25tfidf)
freq_matrix <- data.frame(ST = colnames(temp_frequency),
                          Freq = colSums(temp_frequency))

# Convert to json
json_lda <- LDAvis::createJSON(
  phi = phi,
  theta = theta,
  vocab = vocab,
  doc.length = doc_length,
  term.frequency = freq_matrix$Freq
)

# Open server for visualization
serVis(json_lda, open.browser = TRUE)
```
Top Terms:
- Group 1: regression, computer, distributions, algorithms (regression topic)
- Group 2: applications, theorem, proofs, time, algorithms, theoretical (proofs topic but applications term throws that off)
- Group 3: analysis, systems, computing, communication, business, project, visualization, management (data analysis topic that focuses on visualizing and communicating results)
- Group 4: mathematical, theory, random, multiple, simple (math topic not sure what on)
- Group 5: estimation, information, graphs, networks, matrix, text, big, sets, (estimation in statistics / information theory?)

Top terms for $\beta$
```{r}
uni_topics <- tidy(lda.out,matrix="beta")

top_terms <- uni_topics %>%
 group_by(topic) %>%
 top_n(10, beta) %>%
 ungroup() %>%
 arrange(topic, -beta) %>%
  mutate(term = reorder(term, beta))

top_terms %>%
 ggplot(aes(term, beta, fill = factor(topic))) +
 geom_col(show.legend = FALSE) +
 facet_wrap(~ topic, scales = "free") +
 coord_flip()
```

#### 8 Group Solution

```{r 8 Group TF-IDF Weighting Vis}
# Fit largest model
lda.out <- LDA(dtm_25tfidf, 8, method = "Gibbs",control = list(seed = 87460945))

# Find required quantities
phi <- posterior(lda.out)$terms %>% as.matrix
theta <- posterior(lda.out)$topics %>% as.matrix
vocab <- colnames(phi)
doc_length <- vector()
for (i in 1:length(corpus)) {
  temp <- paste(corpus[[i]]$content, collapse = ' ')
  doc_length <-
    c(doc_length, stri_count(temp, regex = '\\S+'))
}
temp_frequency <- as.matrix(dtm_25tfidf)
freq_matrix <- data.frame(ST = colnames(temp_frequency),
                          Freq = colSums(temp_frequency))

# Convert to json
json_lda <- LDAvis::createJSON(
  phi = phi,
  theta = theta,
  vocab = vocab,
  doc.length = doc_length,
  term.frequency = freq_matrix$Freq
)

# Open server for visualization
 serVis(json_lda, open.browser = TRUE)
```
Top Terms:
- Group 1: applications, regression, distributions, generalized, algorithms, probability (GLMs topic)
- Group 2: mathematical, theory, problems, random, multiple, method, probability, intervals, variance (probability thoery?)
- Group 3: proofs theorem (mathematical proofs topic)
- Group 4: analysis, business, computin, practical, project, research, realworld, quantitative, managment (business data analysis with project)
- Group 5: systems, management, visualization (data visualziation topic)
- Group 6: estimation, information, grpahs, discrete, field, databases (too noisey to interpret)
- Group 7: sets, applications, clustering, image, derivatives, unsupervised (unsupervised learning topic maybe some applications to image data)
- Group 8: big, reading, error, projects, written, network, matrix, graphical (pretty hard to tell)

Top terms for $\beta$
```{r}
uni_topics <- tidy(lda.out,matrix="beta")

top_terms <- uni_topics %>%
 group_by(topic) %>%
 top_n(10, beta) %>%
 ungroup() %>%
 arrange(topic, -beta) %>%
  mutate(term = reorder(term, beta))

top_terms %>%
 ggplot(aes(term, beta, fill = factor(topic))) +
 geom_col(show.legend = FALSE) +
 facet_wrap(~ topic, scales = "free") +
 coord_flip()
```
#### 10 Group Solution

```{r 10 Group TF-IDF Weighting Vis}
# Fit largest model
lda.out <- LDA(dtm_25tfidf, 10, method = "Gibbs",control = list(seed = 87460945))

# Find required quantities
phi <- posterior(lda.out)$terms %>% as.matrix
theta <- posterior(lda.out)$topics %>% as.matrix
vocab <- colnames(phi)
doc_length <- vector()
for (i in 1:length(corpus)) {
  temp <- paste(corpus[[i]]$content, collapse = ' ')
  doc_length <-
    c(doc_length, stri_count(temp, regex = '\\S+'))
}
temp_frequency <- as.matrix(dtm_25tfidf)
freq_matrix <- data.frame(ST = colnames(temp_frequency),
                          Freq = colSums(temp_frequency))

# Convert to json
json_lda <- LDAvis::createJSON(
  phi = phi,
  theta = theta,
  vocab = vocab,
  doc.length = doc_length,
  term.frequency = freq_matrix$Freq
)

# Open server for visualization
 serVis(json_lda, open.browser = TRUE)
```
Top Terms
- Group 1: algorithms, probability, distributions, expectation, method, systems, generating, theorems, computation (definitely statistics though algorithms and computation make me think some kind of computational statistics)
- Group 2: trees, applications, graphs, discrete, differential, random, regression, systems (graph theory course or CARTs)
- Group 3: regression, analysis, applications, theorem, theoretical, iterative, generalized, confience (regression/GLM topic)
- Group 4: mathematical, theory, multiple, analysis, abstraction, finite, nonlinear, monte, carlo, simulationased, processing (mathematical simulation topic)
- Group 5: business. management, analysis, systems, decision, quantitative, project, computing (data analysis topic in business)
- Group 6: computer, information, text, big, web, document, graphical, deep, humanities, analysis (working with text as data)
- Group 7: estimation, field, graphs, ability, binary, neural (neural networks topic)
- Group 8: visualization, clustering, prediction, markov, practical, complex, regularization, handson, segmentation, dimensionality, forecasting (data visualization ?)\
- Group 9: realword, matrices, image, testing, construction, automata, applied, code, determinants (applied linear algebra)
- Group 10: presentation, conditional, series, bayesian, value, eigenvectors, eigenvalues (tough to say)

Top terms for $\beta$
```{r}
uni_topics <- tidy(lda.out,matrix="beta")

top_terms <- uni_topics %>%
 group_by(topic) %>%
 top_n(10, beta) %>%
 ungroup() %>%
 arrange(topic, -beta) %>%
  mutate(term = reorder(term, beta))

top_terms %>%
 ggplot(aes(term, beta, fill = factor(topic))) +
 geom_col(show.legend = FALSE) +
 facet_wrap(~ topic, scales = "free") +
 coord_flip()
```

Overall results are quite varied though there are certainly some shared topics between all the models. All models agree on some sort of topic applying Data Science in business setting. This topic is also mixed with a visualization topic. This occurred 8 times between all the models. Next mathematics/statistics occurring as a topic 4 times. They always appear together. Both machine learning and GLMs were popular topics and appear 4 times.

The remaining topics were less frequent and slightly more nosiey. There is some kind of topic involving modeling or applications of models that occurred 3 times. Text and data mining occurred 3 times. Proofs occurred 3 times and probability theory occurred twice though these topics often occurred together as well so it would not be a stretch to combine them. A graph/set theory topic emerged twice. Some sort of big data topic came up twice. Lastly, neural networks, linear algebra, data sampling/experiment design, and databases came up once.

## n-grams
n-grams allow us to look at different combinations of words and see how they generate topics. I will be looking at bi-grams which means combinations of two words.
```{r bigrams}
library(quanteda)

# Loading in data
load("../data/RObjects/degree_corpus.RData")

# Create corpus object
ds <- DataframeSource(degree_corpus)
corpus <- Corpus(ds)

# Remove punctuation
corpus <- tm_map(corpus, content_transformer(removePunctuation))
# Send everything to lower case
corpus <- tm_map(corpus, content_transformer(tolower))
# Remove stopwords
corpus <- tm_map(corpus,
                 content_transformer(removeWords),
                 stopwords("english"))
# Remove whitespace
corpus <- tm_map(corpus, stripWhitespace)
# Remove numbers
corpus <- tm_map(corpus, content_transformer(removeNumbers))
# Remove custom stop words
remove_words <-
  c("include","methods","topics","design","functions","emphasis","language","introduction","languages","performance","experience","course","science","techniques","variables","number","department","tools","fundamental","also","major","modern","issues","used","methods","using","case","architecture","covered","credit","basic","cosc","granted","use","solutions","data","will","students","fall","spring","important","one","considered","stacks","offers","types","may","held","former","honours","faculty","related","enter","review","enrolment","exercises","summer","need","offered","social","digital","terms","real","concepts","understanding","can","including","programs","program","recommended","examples","introduced","large","search","relations","key","etc","reasoning","intended","fws","general","restricted","version","two","comp","well","rich","intended","required","internet","recent","phys","sciences","covers","year","selected","renewal","explored","csch","principles","practice","development","studies","security","provides","advanced","instruction","discussed","processes","death","lower","high","crncr","taken","efficient","includes","core","retrieval","class","within","present","option","interested","together","session","week","new","order","tables","small","suitable","wide","without","good","introduces","assignments","current","thinking","completed","basics","essential","gain","effective","file","three","many","classes","extensive","tasks","work","meaningful","first","creating","elementary","image"
  )
corpus <- tm_map(corpus, function(x) {
  removeWords(x, remove_words)
})

quan_cor <- quanteda::corpus(corpus)
quan_cor <- tokens(quan_cor, ngrams = 2)%>%
                          tokens_ngrams(n=2)

dtm_bi <- as.DocumentTermMatrix(dfm(quan_cor),weighting = weightTf)
inspect(dtm_bi)
```
Since the 8 group solution yielded the best results before let's try it out on this.

```{r bigram 8 sol lda}
# Fit largest model
lda.out <- LDA(dtm_bi, 8, method = "Gibbs",control = list(seed = 87460945))

# Find required quantities
phi <- posterior(lda.out)$terms %>% as.matrix
theta <- posterior(lda.out)$topics %>% as.matrix
vocab <- colnames(phi)
doc_length <- vector()
for (i in 1:length(corpus)) {
  temp <- paste(corpus[[i]]$content, collapse = ' ')
  doc_length <-
    c(doc_length, stri_count(temp, regex = '\\S+'))
}
temp_frequency <- as.matrix(dtm_bi)
freq_matrix <- data.frame(ST = colnames(temp_frequency),
                          Freq = colSums(temp_frequency))

# Convert to json
json_lda <- LDAvis::createJSON(
  phi = phi,
  theta = theta,
  vocab = vocab,
  doc.length = doc_length,
  term.frequency = freq_matrix$Freq
)

# Open server for visualization
 serVis(json_lda, open.browser = TRUE)
```
Immediately having bigrams helps us gain intuition about our terms. For example support and vector are being put together for SVMs but previously I had been interpreting vector in the linear algebra sense.

Top terms:
- Group 1: math, stat, graph theory, linear equations, analysis variances, model selection (definitely some competing topics. Since math, stats, and graph theory are the highest I will opt for a graph theory topic)
- Group 2: conditional probability, limit theorem, permutation combinations, loglinear models, moment generating (this is a probability theory topic)
- Group 3: estimation hypothesis, law numbers, sequence series, machine learning, algorithms greedy, statistical analysis (probably intro machine learning or statistical learning since machine learning and estimation hypothesis are tied)
- Group 4: linear regression, differential equations, logsitic regression, linear models, neural networks (this is still likely a glm topic though the NN throw it off)
- Group 5: additive models, machine learning, operating system, oral written, staitstical software (another machine learning topic though too noisey)
- Group 6: limits continuity, central limit, lagrange multiplies, least squares, time series, hypothesis testing (more probability theory?)
- Group 7: supper vector, markov chains, vector machines, maximum likelihood (MC or SVM topic)
Group 8: decision making, regression classification, computer programming (again ML topic)

Unforunately while the terms themselves are more informative they don't do well for interpreting topics.

## Collocations
Collocations are terms that have a higer probability of appearing together than they do individually. Rather than just have a model of singular terms or just have a model of bigrams we can have a model that encorporates the most common singular terms, bigrams, and trigrams.

```{r}
# Loading in data
load("../data/RObjects/degree_corpus.RData")

# Create corpus object
ds <- DataframeSource(degree_corpus)
corpus <- Corpus(ds)

# Remove punctuation
corpus <- tm_map(corpus, content_transformer(removePunctuation))
# Send everything to lower case
corpus <- tm_map(corpus, content_transformer(tolower))
# Remove stopwords
corpus <- tm_map(corpus,
                 content_transformer(removeWords),
                 stopwords("english"))
# Remove whitespace
corpus <- tm_map(corpus, stripWhitespace)
# Remove numbers
corpus <- tm_map(corpus, content_transformer(removeNumbers))
# Remove custom stop words
remove_words <-
  c("include","methods","topics","design","functions","emphasis","language","introduction","languages","performance","experience","course","science","techniques","variables","number","department","tools","fundamental","also","major","modern","issues","used","methods","using","case","architecture","covered","credit","basic","cosc","granted","use","solutions","data","will","students","fall","spring","important","one","considered","stacks","offers","types","may","held","former","honours","faculty","related","enter","review","enrolment","exercises","summer","need","offered","social","digital","terms","real","concepts","understanding","can","including","programs","program","recommended","examples","introduced","large","search","relations","key","etc","reasoning","intended","fws","general","restricted","version","two","comp","well","rich","intended","required","internet","recent","phys","sciences","covers","year","selected","renewal","explored","csch","principles","practice","development","studies","security","provides","advanced","instruction","discussed","processes","death","lower","high","crncr","taken","efficient","includes","core","retrieval","class","within","present","option","interested","together","session","week","new","order","tables","small","suitable","wide","without","good","introduces","assignments","current","thinking","completed","basics","essential","gain","effective","file","three","many","classes","extensive","tasks","work","meaningful","first","creating","elementary","image"
  )
corpus <- tm_map(corpus, function(x) {
  removeWords(x, remove_words)
})

dtm_col <- DocumentTermMatrix(corpus)

# convert dtm into sparse matrix
dtm_col_sparse <- Matrix::sparseMatrix(i = dtm_col$i, j = dtm_col$j, 
                           x = dtm_col$v, 
                           dims = c(dtm_col$nrow, dtm_col$ncol),
                           dimnames = dimnames(dtm_col))

# calculate co-occurrence counts
coocurrences <- t(dtm_col_sparse) %*% dtm_col_sparse
# convert into matrix
collocates <- as.matrix(coocurrences)
as.data.frame(collocates)
```

We now have a co-occurence matrix. We can now use a function defined [here](https://ladal.edu.au/coll.html) to show which words are strongly associated with other words. Let's try machine, the obvious strong association we should see is learning.

This currently doesn't work but the visualizations at https://ladal.edu.au/coll.html look very promising. If we can get it to work we can have LDA models like this https://towardsdatascience.com/6-tips-to-optimize-an-nlp-topic-model-for-interpretability-20742f3047e2 which is really cool because it has best bigrams, trigrams, and singular terms which makes for really intepretable topics.

```{r}
# load function for co-occurrence calculation
source("https://slcladal.github.io/rscripts/calculateCoocStatistics.R")
# define term
coocTerm <- "machine"
# calculate co-occurrence statistics
coocs <- calculateCoocStatistics(coocTerm, dtm_col_sparse, measure="LOGLIK")
coocdf <- coocs %>%
  as.data.frame() %>%
  dplyr::mutate(CollStrength = coocs,
                Term = names(coocs)) %>%
  dplyr::filter(CollStrength > 30)

ggplot(coocdf, aes(x = reorder(Term, CollStrength, mean), y = CollStrength)) +
  geom_point() +
  coord_flip() +
  theme_bw() +
  labs(y = "")
```



## References

A topic model for movie reviews. (n.d.). Retrieved March 20, 2023, from  
&nbsp;&nbsp;&nbsp;&nbsp;https://ldavis.cpsievert.me/reviews/reviews.html 

Blei, D. M., Ng, A. Y., & Jordan, M. I. (2003). Latent dirichlet allocation. Journal of
machine Learning 
&nbsp;&nbsp;&nbsp;&nbsp;research, 3(Jan), 993-1022.

Fernandes, E. (2019, November 2). Text preprocessing for NLP and Machine Learning Using R.
DataMathStat. Retrieved March 20, 2023, from  &nbsp;&nbsp;&nbsp;&nbsp;https://datamathstat.wordpress.com/2019/10/25/text-preprocessing-for-nlp-and-machine-learning-using-r/ 

Gandrud, C. (2015, May 8). A Link Between topicmodels LDA and LDAvis. R. Retrieved March 20, &nbsp;&nbsp;&nbsp;&nbsp;2023, from https://www.r-bloggers.com/2015/05/a-link-between-topicmodels-lda-and-ldavis/ 

Grimmer, J., Roberts, M. E., &amp; Stewart, B. M. (2022). Text as data: A New Framework for Machine 
&nbsp;&nbsp;&nbsp;&nbsp;Learning and the Social Sciences. Princeton University Press. 

Meza, D. (2015, July 20). Topic modeling using R Â· Knowledger. knowledgeR. Retrieved March 20, &nbsp;&nbsp;&nbsp;&nbsp;2023,from https://knowledger.rbind.io/post/topic-modeling-using-r/ 

PleplÃ©, Q. (n.d.). Perplexity To Evaluate Topic Models. qpleple. Retrieved March 20, 2023, from 
&nbsp;&nbsp;&nbsp;&nbsp;http://qpleple.com/perplexity-to-evaluate-topic-models/ 

Sievert, C., & Shirley, K. (2014, June). LDAvis: A method for visualizing and interpreting topics. In Proceedings of the workshop on interactive language learning, visualization, and interfaces (pp. 63-70).

Silge, J., Robinson, D., & ProQuest (Firm). (2017). Text mining with R: A tidy approach (First ed.). &nbsp;&nbsp;&nbsp;&nbsp;O'Reilly Media.

Tufts, C. (n.d.). The Little Book of LDA. Latest Posts â Mining the Details. Retrieved March 20, 2023, &nbsp;&nbsp;&nbsp;&nbsp;from https://miningthedetails.com/LDA_Inference_Book/lda-inference.html 


https://ladal.edu.au/coll.html



