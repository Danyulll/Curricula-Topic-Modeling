---
title: "Curricula-Topic-Modeling"
---

## Introduction to Topic Modeling and LDA

### Terminology

Before beginning there are three important preliminary definitions one will need in any text analysis:

-   Document: a distinct text object that one wishes to analyze. This could be a paper, a paragraph, etc.
-   Term: an individual word in a given document.
-   Corpus: The set of all documents.

### Topic Models

Topic Modeling is a statistical model that attempts to cluster the words found in a document into various "topics". The hope is that these topics would capture some underlying subject contained within the text. It is important to note that Topic Models are fuzzy in the sense that documents are assumed to be comprised of multiple topics with varying degrees of membership to each.

### Latent Dirichlet Allocation

Perhaps the most famous topic model is *Latent Dirichlet Allocation* (LDA) (Blei et al., 2003). It is a three-level hierarchical Bayesian model that defines a collection of documents as a random mixture over some underlying topic set where each topic is itself a distribution over our vocabulary.

LDA works by assuming a data generating process (DGP) for our documents and then employs inference to discover the most likely parameters (Grimmer et al., 2022).

Blei, Ng, and Jordan define the following DGP (2003):

1.  Choose $N \sim$ Poisson($\xi$)
2.  Choose $\theta \sim$ Dir($\alpha$)
3.  For each of the $N$ words $w_n$:
    1.  Choose a topic $z_n\sim$Multinomial($\theta$)
    2.  Choose a word $w_n$ from $p(w_n|z_n,\beta)$, a multinomial probability conditioned on the topic $z_n$

For the following analysis we estimate the parameters using the *collapsed Gibbs sampling method*, though it is important to note there are others that yield varied results. To further complicate things, when fitting LDA in R one must predefine the number of topics for the model. Finding a good estimate for the number of topics is paramount and many methods are explored.

Once a model is fitted, it is common to extract 2 matrices $\Phi$ and $\Theta$. These are also commonly referred to as $\beta$ and $\gamma$ respectively. Both are given by the priors of LDA. $\Phi$ represents the probability mass over the terms. $\Theta$ represents the probability mass function over the topics or the per-document-per-topic probabilities.


## Feature Extraction and Preprocessing

### Document-Term Matrices

There are many methods of feature extraction from text, we opt for the *bag of words* model. To construct this model simply define a common set of words shared between documents and store a count of word appearances for each document. Commonly this is stored as a *Document-Term Matrix* (DTM), for example, given the documents:

::: {#tbl-panel layout-ncol="2"}
| Document 1   |
|--------------|
| "I am happy" |

| Document 2 |
|------------|
| "I am sad" |

Corpus
:::

the corresponding DTM would be

|           | I   |  am | happy | sad |
|-----------|:----|----:|:-----:|:---:|
| **Doc 1** | 1   |   1 |   1   |  0  |
| **Doc 2** | 1   |   1 |   0   |  1  |

In R we can us the tm package to create our Corpus and DTM objects. For these examples scrapped Data Science course information, from various universities, will be preprocessed.

First this data will be examined from a degree pathway perspective with the documents being concatenated course descriptions for entire curricula. These curricula are constructed for each university by sampling courses from each university's Data Science course calendar.

```{r Libraries, echo=FALSE}
# Load Libraries
library(tm)
library(topicmodels)
library(ggplot2)
library(ldatuning)
library(LDAvis)
library(dplyr)
library(stringi)
library(tidytext)
library(tidyr)
library(wordcloud)
library(graphics)
```


```{r Load in Data}
# Loading in data
load("../data/RObjects/degree_corpus.RData")

# Examining which universities are represented 
print(degree_corpus$doc_id)
```

```{r Constructing Corpus}
ds <- DataframeSource(degree_corpus)
corpus <- Corpus(ds)
inspect(corpus[1])
```

As shown the first element of the corpus is from Western University and contains all course description terms from a given pathway through their Data Science calendar.

### Complexity Reduction

Raw text data lends itself to difficult analysis. NLP posses several best practices for cleaning text:

-   Punctation/Number Removal: deleting any non alphabetic characters.
-   Lowercasing: sending all words to lowercase.
-   Stopword Removal: stopwords are words used often in sentences that give little to no information, i.e., atricles such as the, a, etc.
-   Stemming: truncating the ends of words so that they share a common base, i.e., fishing and fishes would be transformed ot fish.
-   Tokenizing: dividing a document into a set of individual words

```{r Text Preprocessing}
# Remove punctuation
corpus <- tm_map(corpus, content_transformer(removePunctuation))
# Send everything to lower case
corpus <- tm_map(corpus, content_transformer(tolower))
# Remove stopwords
corpus <- tm_map(corpus, content_transformer(removeWords), 
          stopwords("english"))
# Remove whitespace
corpus <- tm_map(corpus, stripWhitespace)
# Remove numbers
corpus <- tm_map(corpus, content_transformer(removeNumbers))
# Remove custom stop words
corpus <- tm_map(corpus, function(x) {removeWords(x,c("data","will","students","fall","spring","important","one","considered","stacks","offers","types","may","held","former","honours","faculty","related","enter","review","enrolment","exercises","summer","need","offered"))})
# Stemming
corpus <- tm_map(corpus, stemDocument)

inspect(corpus[1])
```

Now we may construct our DTM,

```{r DTM Construction}
# Create DTM object
dtm <- DocumentTermMatrix(corpus)
inspect(dtm[,1:9])
```

### TF-IDf

If one wishes to further reduce complexity there exist various heuristics for term inclusion. One such heuristic is *term frequency inverse document frequency* (TF-IDF). The intuition for TF-IDF is that extremely frequent words hold language together but do not provide great insight of topic or context. Meanwhile rare words contain a plethora of information but lack the frequency needed to be used in generalizations. Therefore words somewhere in the middle of these extremes ought to be included (Grimmer et al., 2022).

The quantity itself is the multiplication of the *term frequency* (TF) and *inverse document drequency* (IDF). TF is simply the number of times the term $j$ appears in a given document. IDF is defined as

$$idf(term)=ln\left(\frac{{\text{\# of doucments}}}{{\text{\# documents containing term j}}}\right)$$

IDF is a penalty term that adjusts term frequency based on the term's rarity (Silge & Robinson, 2017).

```{r TF-IDF weighting}
tfidf_dtm <- weightTfIdf(dtm)
inspect(tfidf_dtm[,1:5])
```

Now our DTM contains TF-IDF scores as the entries as opposed to just TF. We can now filter away terms below a certain threshold of TF-IDF. A common approach is to filter terms away below the median.

```{r TF-IDF Median Calc}
tfidf_mat <- as.matrix(tfidf_dtm)
median(tfidf_mat[tfidf_mat > 0])
```

```{r Filter TF-IDF below median}
tfidf_dtm_r <- tfidf_dtm[,tfidf_dtm$v > 0.0022]
```

```{r TF-IDF DTM Dimensionality}
# Dimensions before reduction
dim(tfidf_dtm)

# Dimensions after reduction
dim(tfidf_dtm_r)
```

With this our dimensionality has sufficiently reduced. It is important to note when we fit a LDA model the topicmodels package in R requires us to have term frequencies as our DTM entries so we must convert back to a TF weighting.

```{r Convert back to TF weighting}
# Get our list of tfidf terms       
terms <- tfidf_dtm_r$dimnames$Terms

# Filter our original dtm
dtm <- dtm[,which(dtm$dimnames$Terms %in% terms)]

# Check to make sure we filtered the correct terms
sum(dtm$dimnames$Terms == terms) == 702

```

## Exploring Degree Pathways

### Topic Number Discovery

LDA, like many unsupervised learning methods, is extremely dependent on its parameters. One such parameter that requires meticulous tuning is the number of topics. In the literature there exist many metrics for discovering the optimal number of topics with several being readily available in R.

#### Perplexity

Perplexity is a measure of a model's ability to generalize to unseen data. It is the log-likelihood of a a test set of data and is given by PleplÃ©:

$$\ell(w)=\text{log}p(w|\Phi,\alpha)=\sum_d \text{log} p(w_d|\Phi,\alpha)$$

- $w_d$: the unseen document
- $\Phi$: the matrix for the topics
- $\alpha$: the hyperparameter for the topic distributions

The lower our perplexity score the better. We can use 3-fold cross validation and plot our perplexities for various topic numbers.

```{r Perplexity 3-fold CV, cache=TRUE}
set.seed(87460945)

# Calculate folds
idxs <- sample(seq_len(9))
folds <- split(idxs, rep(1:3, each = 3, length.out = 9))

# Define number of topics
topics <- seq(2, 50, 1)

# Create data frame for storing results
results <- data.frame()

# Perform cross validation
for (k in topics) {
  scores <- c()
  for (i in 1:3) {
    test_idx <- folds[[i]]
    train_idx <- setdiff(unlist(folds, use.names = FALSE), test_idx)

    test <- dtm[test_idx, ]
    train <- dtm[train_idx, ]

    LDA.out <- LDA(dtm, k, method = "Gibbs")
    p <- perplexity(LDA.out, newdata = test)
    scores <- c(scores, p)
  }
  temp <- data.frame("K" = k, "Perplexity" = mean(scores))
  results <- rbind(results, temp)
}

# Plot Perplexity vs. K
ggplot(results, aes(x=K, y=Perplexity)) + geom_line() + ggtitle("Perplexity vs. Number of Topics K")+
  theme(plot.title = element_text(hjust = 0.5))
```

```{r min Perplexity}
# Retrieving K for minimum Perplexity
results[which.min(results$Perplexity),"K"]
```

Looks like in terms of Perplexity 8 is our optimal number of topics.

#### Coherence

**TO DO Do this when I have time**

#### ldatuning

The ldatuning package provides us with several metrics to evaluate the number topics all of which rely on find extrema. We seek to minimize Arun2010 and CaoJuan2009 and maximize Deveaud2014 and Griffiths2004.

```{r ldatuning k 40,cache=TRUE}
# Find number of topics
result <- FindTopicsNumber(
  dtm,
  topics = seq(from = 2, to = 40, by = 1),
  metrics = c("Griffiths2004", "CaoJuan2009", "Arun2010", "Deveaud2014"),
  method = "Gibbs",
  control = list(seed = 77),
  mc.cores = 2L,
)
# Plot metrics
suppressWarnings(FindTopicsNumber_plot(result))
```

It seems no topic numbers over 21 are viable. Let us zoom in on the 2-21 range.

```{r ldatuning k 21,cache=TRUE}
# Find number of topics
result <- FindTopicsNumber(
  dtm,
  topics = seq(from = 2, to = 21, by = 1),
  metrics = c("Griffiths2004", "CaoJuan2009", "Arun2010", "Deveaud2014"),
  method = "Gibbs",
  control = list(seed = 77),
  mc.cores = 2L,
)
# Plot metrics
suppressWarnings(FindTopicsNumber_plot(result))
```

For our minima we see CaoJuan2009 suggests 6 and 9 while Arun2019 promots 19. Examining our maxima we find Deveaud2014 is uninterpretable and Griffiths2004 recommends 10 and 20. This provides us with a good range of topic numbers to explore, $k = 6,8,9,10,19,20$.

#### Visualizing topics with LDAvis

Rather than individually fit each model model for our topic numbers the R package LDAvis creates a fantastic interactable visualization of our LDA model. LDAvis tries to answer 3 questions (Sievert & Shirley, 2014): 

- How prevalent is each topic? 
- How do the topics relate to each other? 
- How do the topics relate to each other?

```{r LDAvis, cache=TRUE}
# Fit largest model
lda.out <- LDA(dtm, 19, method = "Gibbs")

# Find required quantities
phi <- posterior(lda.out)$terms %>% as.matrix
theta <- posterior(lda.out)$topics %>% as.matrix
vocab <- colnames(phi)
doc_length <- vector()
for (i in 1:length(corpus)) {
  temp <- paste(corpus[[i]]$content, collapse = ' ')
  doc_length <-
    c(doc_length, stri_count(temp, regex = '\\S+'))
}
temp_frequency <- as.matrix(dtm)
freq_matrix <- data.frame(ST = colnames(temp_frequency),
                          Freq = colSums(temp_frequency))

# Convert to json
json_lda <- LDAvis::createJSON(
  phi = phi,
  theta = theta,
  vocab = vocab,
  doc.length = doc_length,
  term.frequency = freq_matrix$Freq
)

# Open server for visualization
serVis(json_lda, out.dir = "../LDAvis", open.browser = FALSE)
```


  <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
  
  <script src="./LDAvis/d3.v3.js"></script>
  <script src="./LDAvis/ldavis.js"></script>
  <link rel="stylesheet" type="text/css" href="./LDAvis/lda.css">



  <div id = "lda"></div>
  <script>
    var vis = new LDAvis("#lda", "./LDAvis/lda.json");
  </script>

To interpret this visualization we fist need to define several quantities:

*Relevance* - Sievert and Shirley propose relevance  as a metric to rank terms based on their usefulness for interpreting topics (2014). They define it as follows:

$$r(w,k|\lambda)=\lambda \text{log}(\phi_{kw}+(1-\lambda)\text{log}\left( \frac{\phi_{kw}}{p_w}\right)$$

- $\phi_{kw}$: the probability of term $w$ for topic $k$.
- $p_w$: the marginal probability of term $w$ in the corpus.
- $\lambda$: a weight parameter such that $0 \leq \lambda \leq 1$.
- $\text{log}\left( \frac{\phi_{kw}}{p_w}\right)$: the lift of a term.

A $\lambda$ of 1 orders terms based on their topic-specific probability whereas a $\lambda$ of 0 orders term based on their lift. Sievert and Shirley suggest $\lambda = 6$. The higher this metric the better.

*Lift* - As seen above Lift is the ratio of a term's probability in a given topic to its marginal probability. The metric downweights terms that are frequent throughout an entire corpus (Sievert & Shirley 2014).

*Saliency* - Introuced by Chuang, Manning, and Heer, it is defined as follows (2012):
$$\begin{align} saliency(w) &= P(W) \times distinctiveness(w) \\
distinctiveness(w) &= \sum_T P(T|w)\text{log}\frac{P(T|w)}{P(T)}
\end{align}$$

- $P(T|w)$: the probability a word $w$ was genereted by a topic $T$
- $P(T)$: the marginal probability of the topic, i.e., the probability a word was produced b topic $T$

The general idea is that distinctiveness is the Kullback-Leibler divergence between $P(T|w)$ and $P(T)$ and informs us how useful a $w$ is for determining a generating topic. Multiplying this by $P(w)$ yields saliency; a metric that ranks terms in their usefulness in identifying topics.

With these we can understand our visuals:

- Topic circles: circles are drawn equal to the number of topics. Their areas are proportional to the estimated number of words produced by that topic. The centers of the circles are determined by computing the distances between topics which are then projected onto a 2D plane. The larger a circle is, the more prevalent it is.

-   Red Bars: the estimated number of times a term was created by a  topic. When you select a circle (topic) the red bars for the most relevant terms will be highlighted.

-   Blue Bars: each bar represents the frequency of each term. When no topic is selected the blue bars for the most salient terms are displayed. When a topic is selected the blue bars are the frequency of the most relevant terms.

-   Topic-Term Circles: When you highlight a term the circles' areas will change. These circles' areas are proportional to the number of times a topic generated that specific term.

If $\lambda=1$ terms are ranked in decreasing order of their topic-specific probability. $\lambda=0$ ranks terms solely by their lift. The optimal $\lambda$ is $\lambda=0.6$.

### Inspecting $\beta$

Given our optimal model we may now move onto exploring what words comprise each of the topics. To do so we first plot the terms with the highest $\beta$ values and then move onto wordclouds. Recall $\beta$ provides us with the per-topic-per-word probabilities. These are the probability a term was created by a topic.

```{r Graph Beta University Level}
#| fig-width: 10
#| fig-height: 10
uni_topics <- tidy(lda.out,matrix="beta")

top_terms <- uni_topics %>%
 group_by(topic) %>%
 top_n(9, beta) %>%
 ungroup() %>%
 arrange(topic, -beta) %>%
  mutate(term = reorder(term, beta))

top_terms %>%
 ggplot(aes(term, beta, fill = factor(topic))) +
 geom_col(show.legend = FALSE) +
 facet_wrap(~ topic, scales = "free") +
 coord_flip()
```

Examining word clouds may also prove fruitful.

```{r wordcloud beta topics in university}

set.seed(87460945)

top_terms <- uni_topics %>%
 group_by(topic) %>%
 top_n(20, beta) %>%
 ungroup() %>%
 arrange(topic, -beta) %>%
  mutate(term = reorder(term, beta))

# for (i in 1:20) {
# df <- subset(as.data.frame(top_terms),topic==i)  
# wordcloud(df$term, freq = df$beta,colors= c("indianred1","indianred2","indianred3","indianred"))
# title(paste("Topic ",i), font.main = 1)
# }
```
::: {.panel-tabset}

# Topic 1
```{r, echo=FALSE}
df <- subset(as.data.frame(top_terms),topic==1)
suppressWarnings(wordcloud(df$term, freq = df$beta,colors= c("indianred1","indianred2","indianred3","indianred"))) 

```

# Topic 2
```{r, echo=FALSE}
df <- subset(as.data.frame(top_terms),topic==2)
suppressWarnings(wordcloud(df$term, freq = df$beta,colors= c("indianred1","indianred2","indianred3","indianred")))
```

# Topic 3
```{r, echo=FALSE}
df <- subset(as.data.frame(top_terms),topic==3)
suppressWarnings(wordcloud(df$term, freq = df$beta,colors= c("indianred1","indianred2","indianred3","indianred")))
```

# Topic 4
```{r, echo=FALSE}
df <- subset(as.data.frame(top_terms),topic==4)
suppressWarnings(wordcloud(df$term, freq = df$beta,colors= c("indianred1","indianred2","indianred3","indianred")))
```

# Topic 5
```{r, echo=FALSE}
df <- subset(as.data.frame(top_terms),topic==5)
suppressWarnings(wordcloud(df$term, freq = df$beta,colors= c("indianred1","indianred2","indianred3","indianred")))
```

# Topic 6
```{r, echo=FALSE}
df <- subset(as.data.frame(top_terms),topic==6)
suppressWarnings(wordcloud(df$term, freq = df$beta,colors= c("indianred1","indianred2","indianred3","indianred")))
```

# Topic 7
```{r, echo=FALSE}
df <- subset(as.data.frame(top_terms),topic==7)
suppressWarnings(wordcloud(df$term, freq = df$beta,colors= c("indianred1","indianred2","indianred3","indianred")))
```

# Topic 8
```{r, echo=FALSE}
df <- subset(as.data.frame(top_terms),topic==8)
suppressWarnings(wordcloud(df$term, freq = df$beta,colors= c("indianred1","indianred2","indianred3","indianred")))
```


# Topic 9
```{r, echo=FALSE}
df <- subset(as.data.frame(top_terms),topic==9)
suppressWarnings(wordcloud(df$term, freq = df$beta,colors= c("indianred1","indianred2","indianred3","indianred")))
```

# Topic 10
```{r, echo=FALSE}
df <- subset(as.data.frame(top_terms),topic==10)
suppressWarnings(wordcloud(df$term, freq = df$beta,colors= c("indianred1","indianred2","indianred3","indianred")))
```


# Topic 11
```{r, echo=FALSE}
df <- subset(as.data.frame(top_terms),topic==11)
suppressWarnings(wordcloud(df$term, freq = df$beta,colors= c("indianred1","indianred2","indianred3","indianred"))) 

```

# Topic 12
```{r, echo=FALSE}
df <- subset(as.data.frame(top_terms),topic==12)
suppressWarnings(wordcloud(df$term, freq = df$beta,colors= c("indianred1","indianred2","indianred3","indianred")))
```

# Topic 13
```{r, echo=FALSE}
df <- subset(as.data.frame(top_terms),topic==13)
suppressWarnings(wordcloud(df$term, freq = df$beta,colors= c("indianred1","indianred2","indianred3","indianred")))
```

# Topic 14
```{r, echo=FALSE}
df <- subset(as.data.frame(top_terms),topic==14)
suppressWarnings(wordcloud(df$term, freq = df$beta,colors= c("indianred1","indianred2","indianred3","indianred")))
```

# Topic 15
```{r, echo=FALSE}
df <- subset(as.data.frame(top_terms),topic==15)
suppressWarnings(wordcloud(df$term, freq = df$beta,colors= c("indianred1","indianred2","indianred3","indianred")))
```

# Topic 16
```{r, echo=FALSE}
df <- subset(as.data.frame(top_terms),topic==16)
suppressWarnings(wordcloud(df$term, freq = df$beta,colors= c("indianred1","indianred2","indianred3","indianred")))
```

# Topic 17
```{r, echo=FALSE}
df <- subset(as.data.frame(top_terms),topic==17)
suppressWarnings(wordcloud(df$term, freq = df$beta,colors= c("indianred1","indianred2","indianred3","indianred")))
```

# Topic 18
```{r, echo=FALSE}
df <- subset(as.data.frame(top_terms),topic==18)
suppressWarnings(wordcloud(df$term, freq = df$beta,colors= c("indianred1","indianred2","indianred3","indianred")))
```


# Topic 19
```{r, echo=FALSE}
df <- subset(as.data.frame(top_terms),topic==19)
suppressWarnings(wordcloud(df$term, freq = df$beta,colors= c("indianred1","indianred2","indianred3","indianred")))
```

# Topic 20
```{r, echo=FALSE}
df <- subset(as.data.frame(top_terms),topic==20)
suppressWarnings(wordcloud(df$term, freq = df$beta,colors= c("indianred1","indianred2","indianred3","indianred")))
```

:::

### Inspecting $\gamma$

Another interesting question to explore is which topics do universities prioritize? Is one university's Data Science program more heavily biased towards Math? How about Statistics? Recall the $\gamma$ matrix provides us with the per-document-per-topic probabilities. That is, it provides us with a percentage break down of which topics generated each document.

```{r Graph Gamma University Level}
#| fig-width: 10
#| fig-height: 10

uni_topics <- tidy(lda.out,matrix="gamma")

top_terms <- uni_topics %>%
 group_by(topic) %>%
 arrange(topic, gamma)

top_terms %>%
 mutate(topic = reorder(topic, gamma)) %>%
 ggplot(aes(topic, gamma, fill = factor(document))) +
 geom_col(show.legend = FALSE) +
 facet_wrap(~ document, scales = "free") +
 coord_flip()
```

## Inspecting Other Solutions

From our 20 group visualization we see significant overlap on the left clusters. This suggests one of our fewer topic solutions may be more apt. Therefore we opt to visualize the 5, 8, and 10 group solutions. Furthermore, it seems in our word clouds stemming has made interpretability more difficult. Therefore we change our DTM to not include stemming; if similair rooted words appear we can always go back and combine them by hand.

```{r Remaking DTM}
# Loading in data
load("../data/RObjects/degree_corpus.RData")

# Construct corpus object
ds <- DataframeSource(degree_corpus)
corpus <- Corpus(ds)

# Text preprocessing omitting stemming

# Remove punctuation
corpus <- tm_map(corpus, content_transformer(removePunctuation))
# Send everything to lower case
corpus <- tm_map(corpus, content_transformer(tolower))
# Remove stopwords
corpus <- tm_map(corpus, content_transformer(removeWords), 
          stopwords("english"))
# Remove whitespace
corpus <- tm_map(corpus, stripWhitespace)
# Remove numbers
corpus <- tm_map(corpus, content_transformer(removeNumbers))
# Remove custom stop words
corpus <- tm_map(corpus, function(x) {removeWords(x,c("data","will","students","fall","spring","important","one","considered","stacks","offers","types","may","held","former","honours","faculty","related","enter","review","enrolment","exercises","summer","need","offered"))})

# Create DTM
dtm <- DocumentTermMatrix(corpus)
inspect(dtm)
```
```{r Examing TF-IDF once more}
# Find median TF-IDF
tfidf_mat <- as.matrix(tfidf_dtm)
median(tfidf_mat[tfidf_mat > 0])
```

```{r Filter based on TF-IDF once more}
# Filter based on meadian TF-IDF
tfidf_dtm_r <- tfidf_dtm[,tfidf_dtm$v > 0.0022]
```

```{r Revert to original weightings}
# Get our list of tfidf terms       
terms <- tfidf_dtm_r$dimnames$Terms

# Filter our original dtm
dtm <- dtm[,which(dtm$dimnames$Terms %in% terms)]

inspect(dtm)
```

### 5 Group Solution

```{r}
# Fit largest model
lda.out <- LDA(dtm, 5, method = "Gibbs")

# Find required quantities
phi <- posterior(lda.out)$terms %>% as.matrix
theta <- posterior(lda.out)$topics %>% as.matrix
vocab <- colnames(phi)
doc_length <- vector()
for (i in 1:length(corpus)) {
  temp <- paste(corpus[[i]]$content, collapse = ' ')
  doc_length <-
    c(doc_length, stri_count(temp, regex = '\\S+'))
}
temp_frequency <- as.matrix(dtm)
freq_matrix <- data.frame(ST = colnames(temp_frequency),
                          Freq = colSums(temp_frequency))

# Convert to json
json_lda <- LDAvis::createJSON(
  phi = phi,
  theta = theta,
  vocab = vocab,
  doc.length = doc_length,
  term.frequency = freq_matrix$Freq
)

# Open server for visualization
serVis(json_lda, out.dir = "../LDAvis5", open.browser = FALSE)
```

View the 5 group solution [here](https://danyulll.github.io/LDAvis5Sol/).

### 8 Group Solution

```{r}
# Fit largest model
lda.out <- LDA(dtm, 8, method = "Gibbs")

# Find required quantities
phi <- posterior(lda.out)$terms %>% as.matrix
theta <- posterior(lda.out)$topics %>% as.matrix
vocab <- colnames(phi)
doc_length <- vector()
for (i in 1:length(corpus)) {
  temp <- paste(corpus[[i]]$content, collapse = ' ')
  doc_length <-
    c(doc_length, stri_count(temp, regex = '\\S+'))
}
temp_frequency <- as.matrix(dtm)
freq_matrix <- data.frame(ST = colnames(temp_frequency),
                          Freq = colSums(temp_frequency))

# Convert to json
json_lda <- LDAvis::createJSON(
  phi = phi,
  theta = theta,
  vocab = vocab,
  doc.length = doc_length,
  term.frequency = freq_matrix$Freq
)

# Open server for visualization
serVis(json_lda, out.dir = "../LDAvis8", open.browser = FALSE)
```

View the 8 group solution [here](https://danyulll.github.io/LDAvis8Sol-/).

### 10 Group Solution

```{r}
# Fit largest model
lda.out <- LDA(dtm, 10, method = "Gibbs")

# Find required quantities
phi <- posterior(lda.out)$terms %>% as.matrix
theta <- posterior(lda.out)$topics %>% as.matrix
vocab <- colnames(phi)
doc_length <- vector()
for (i in 1:length(corpus)) {
  temp <- paste(corpus[[i]]$content, collapse = ' ')
  doc_length <-
    c(doc_length, stri_count(temp, regex = '\\S+'))
}
temp_frequency <- as.matrix(dtm)
freq_matrix <- data.frame(ST = colnames(temp_frequency),
                          Freq = colSums(temp_frequency))

# Convert to json
json_lda <- LDAvis::createJSON(
  phi = phi,
  theta = theta,
  vocab = vocab,
  doc.length = doc_length,
  term.frequency = freq_matrix$Freq
)

# Open server for visualization
serVis(json_lda, out.dir = "../LDAvis10", open.browser = FALSE)
```

View the 10 group solution [here](https://danyulll.github.io/LDAvis10Sol/).


## References

A topic model for movie reviews. (n.d.). Retrieved March 20, 2023, from  
&nbsp;&nbsp;&nbsp;&nbsp;https://ldavis.cpsievert.me/reviews/reviews.html 

Blei, D. M., Ng, A. Y., & Jordan, M. I. (2003). Latent dirichlet allocation. Journal of
machine Learning 
&nbsp;&nbsp;&nbsp;&nbsp;research, 3(Jan), 993-1022.

Fernandes, E. (2019, November 2). Text preprocessing for NLP and Machine Learning Using R.
DataMathStat. Retrieved March 20, 2023, from  &nbsp;&nbsp;&nbsp;&nbsp;https://datamathstat.wordpress.com/2019/10/25/text-preprocessing-for-nlp-and-machine-learning-using-r/ 

Gandrud, C. (2015, May 8). A Link Between topicmodels LDA and LDAvis. R. Retrieved March 20, &nbsp;&nbsp;&nbsp;&nbsp;2023, from https://www.r-bloggers.com/2015/05/a-link-between-topicmodels-lda-and-ldavis/ 

Grimmer, J., Roberts, M. E., &amp; Stewart, B. M. (2022). Text as data: A New Framework for Machine 
&nbsp;&nbsp;&nbsp;&nbsp;Learning and the Social Sciences. Princeton University Press. 

Meza, D. (2015, July 20). Topic modeling using R Â· Knowledger. knowledgeR. Retrieved March 20, &nbsp;&nbsp;&nbsp;&nbsp;2023,from https://knowledger.rbind.io/post/topic-modeling-using-r/ 

PleplÃ©, Q. (n.d.). Perplexity To Evaluate Topic Models. qpleple. Retrieved March 20, 2023, from 
&nbsp;&nbsp;&nbsp;&nbsp;http://qpleple.com/perplexity-to-evaluate-topic-models/ 

Sievert, C., & Shirley, K. (2014, June). LDAvis: A method for visualizing and interpreting topics. In Proceedings of the workshop on interactive language learning, visualization, and interfaces (pp. 63-70).

Silge, J., Robinson, D., & ProQuest (Firm). (2017). Text mining with R: A tidy approach (First ed.). &nbsp;&nbsp;&nbsp;&nbsp;O'Reilly Media.

Tufts, C. (n.d.). The Little Book of LDA. Latest Posts â Mining the Details. Retrieved March 20, 2023, &nbsp;&nbsp;&nbsp;&nbsp;from https://miningthedetails.com/LDA_Inference_Book/lda-inference.html 


