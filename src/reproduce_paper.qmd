---
title: "paper resources"
format: html
execute: 
  cache: true
---

# Data Preprocessing

Here is code to recreate visualizations found in the paper.

```{r}
# Load Libraries
library(quanteda.textstats)
library(quanteda)
library(tm)
library(topicmodels)
library(ggplot2)
library(ldatuning)
library(LDAvis)
library(dplyr)
library(stringi)
library(stringr)
library(tidytext)
library(tidyr)
library(wordcloud)
library(graphics)
library(slam)
library(gridExtra)
```


```{r Load in Data Total, cache=TRUE}
load("../data/RObjects/degree_corpus_total.RData")
colnames(degree_corpus_total) <- c("doc_id", "text")
degree_corpus_total <- degree_corpus_total[degree_corpus_total$text != "",]
degree_corpus_total <- na.omit(degree_corpus_total)
degree_corpus_total <- degree_corpus_total[!duplicated(degree_corpus_total$doc_id),]
```

```{r Construct Corpus Total, cache=TRUE}
ds <- DataframeSource(degree_corpus_total)
corpus <- Corpus(ds)
inspect(corpus[1])
```

```{r Text Preprocessing Total, cache=TRUE}
# Remove punctuation
corpus <- tm_map(corpus, content_transformer(removePunctuation))
# Send everything to lower case
corpus <- tm_map(corpus, content_transformer(tolower))
# Remove stopwords
corpus <- tm_map(corpus,
                 content_transformer(removeWords),
                 stopwords("english"))
# Remove numbers
corpus <- tm_map(corpus, content_transformer(removeNumbers))

# Remove custom stop words
# remove_words <-
#   c("include","methods","topics","design","functions","emphasis","language","introduction","languages","performance","experience","course","techniques","variables","number","department","tools","fundamental","also","major","modern","issues","used","methods","using","case","architecture","covered","credit","basic","cosc","granted","use","solutions","will","students","fall","spring","important","one","considered","stacks","offers","types","may","held","former","honours","faculty","related","enter","review","enrolment","exercises","summer","need","offered","social","digital","terms","real","concepts","understanding","can","including","programs","program","recommended","examples","introduced","large","search","relations","key","etc","reasoning","intended","fws","general","restricted","version","two","comp","well","rich","intended","required","internet","recent","phys","sciences","covers","year","selected","renewal","explored","csch","principles","practice","development","studies","security","provides","advanced","instruction","discussed","processes","death","lower","high","crncr","taken","efficient","includes","core","retrieval","class","within","present","option","interested","together","session","week","new","order","tables","small","suitable","wide","without","good","introduces","assignments","current","thinking","completed","basics","essential","gain","effective","file","three","many","classes","extensive","tasks","work","meaningful","first","creating","elementary","image","variety","field","engl","skills","evaluation","advances","however","substantial","ece","fields","sucessful","effectively","beyond","explicit","describe","take","earlier","worstcase","obtained","rules","previously","life","allow","abstractions","intensive","agreement","involving","shared","thus","attached","firsthand","partners","provider","remain","entity","given","delves","offerings","available","testingand","vary","school","complete","choice","certain","identify","ides","term","minimum","upon","working","discusses","final","highlevel"
  # )

# This is the new remove words that may mess things up:
remove_words <-
  c("include","methods","topics","design","functions","emphasis","language","introduction","languages","performance","experience","course","techniques","variables","number","department","tools","fundamental","also","major","modern","issues","used","methods","using","case","architecture","covered","credit","basic","cosc","granted","use","solutions","will","students","fall","spring","important","one","considered","stacks","offers","types","may","held","former","honours","faculty","related","enter","review","enrolment","exercises","summer","need","offered","social","digital","terms","real","concepts","understanding","can","including","programs","program","recommended","examples","introduced","large","search","relations","key","etc","reasoning","intended","fws","general","restricted","version","two","comp","well","rich","intended","required","internet","recent","phys","sciences","covers","year","selected","renewal","explored","csch","principles","practice","development","studies","security","provides","advanced","instruction","discussed","processes","death","lower","high","crncr","taken","efficient","includes","core","retrieval","class","within","present","option","interested","together","session","week","new","order","tables","small","suitable","wide","without","good","introduces","assignments","current","thinking","completed","basics","essential","gain","effective","file","three","many","classes","extensive","tasks","work","meaningful","first","creating","elementary","image","variety","field","engl","skills","evaluation","advances","however","substantial","ece","fields","sucessful","effectively","beyond","explicit","describe","take","earlier","worstcase","obtained","rules","previously","life","allow","abstractions","intensive","agreement","involving","shared","thus","attached","firsthand","partners","provider","remain","entity","given","delves","offerings","available","testingand","vary","school","complete","choice","certain","identify","ides","term","minimum","upon","working","discusses","final","highlevel","weeks","interest","passfail","introductory","satisfy","grade","problem"
)

corpus <- tm_map(corpus, function(x) {
  removeWords(x, remove_words)
})

# Remove whitespace
corpus <- tm_map(corpus, stripWhitespace)

inspect(corpus[1])
```

We search for bigrams once again:

```{r Create Bigrams Total, cache=TRUE}
# Inspecting BiGrams table
# corpusq <- quanteda::corpus(corpus)
# 
# corpuz_tokzd <- quanteda::tokens(corpusq)
# 
# BiGrams <- corpuz_tokzd %>% 
#        quanteda::tokens_select(pattern = "^[A-Z]", 
#                                valuetype = "regex",
#                                case_insensitive = TRUE, 
#                                padding = TRUE) %>% 
#        textstat_collocations(min_count = 500, tolower = FALSE,size=2)
# 
# BiGrams
```

```{r Plot Bigrams Total, cache=TRUE}
# Plotting BiGrams
# plot1 <- ggplot(BiGrams, aes(fill=count, y=lambda, x=reorder(collocation, -lambda))) + 
#     geom_bar(position="dodge", stat="identity") + coord_flip()+labs(x= "Bigrams", y = "lambda")
# 
# plot2 <- ggplot(BiGrams, aes(fill=count, y=z, x=reorder(collocation, -z))) + 
#     geom_bar(position="dodge", stat="identity") + coord_flip()+labs(x= "Bigrams", y = "z")
#   
# grid.arrange(plot1, plot2, ncol=2)
```

```{r Add Bigrams Total, cache=TRUE}
# Recreate data frame
cor <- as.list(corpus)
df <- cbind(data.frame("doc_id" = names(cor)),data.frame("text"=t(as.data.frame(cor))))

df <- createBigram(df,"data","science","data_science")
df <- createBigram(df,"data","structures","data_structures")
df <- createBigram(df,"data","analysis","data_analysis")
df <- createBigram(df,"data","mining","data_mining")
# df <- createBigram(df,"text","mining","text_mining")
df <- createBigram(df,"machine","learning","machine_learning")
df <- createBigram(df,"computer","science","computer_science")
df <- createBigram(df,"time","series","time_series")
df <- createBigram(df,"database","systems","database_systems")
df <- createBigram(df,"data","visualization","data_visualization")
df <- createBigram(df,"neural","networks","neural_networks")
df <- createBigram(df,"graph","theory","graph_theory")
df <- createBigram(df,"differential","equations","differential_equations")
df <- createBigram(df,"big","data","big_data")
df <- createBigram(df,"hypothesis","testing","hypothesis_testing")
# df <- createBigram(df,"linear","regression","linear_regression")
# df <- createBigram(df,"regression","models","regression_models")
# df <- createBigram(df,"data","sets","data_sets")
df <- createBigram(df,"text","data","text_data")
df <- createBigram(df,"monte","carlo","monte_carlo")
df <- createBigram(df,"markov","chain","markov_chain")
df <- createBigram(df,"markov","chains","markov_chains")
df <- createBigram(df,"actuarial","science","actuarial_science")
df <- createBigram(df,"exploratory","data","exploratory_data")
# df <- createBigram(df,"r","programming","r_programming")
```

```{r Recreate Corpus Total, cache=TRUE}
ds2 <- DataframeSource(df)
corpus2 <- Corpus(ds2)

remove_words <-
  c("data","science")
corpus2 <- tm_map(corpus2, function(x) {
  removeWords(x, remove_words)
})

dtm <- DocumentTermMatrix(corpus2)
```

```{r Combine Columns Total, cache=TRUE}
dtm <-
  combineCols(c("math", "mathematics", "mathematically", "mathematical"),
              dtm,"mathematics")
dtm <- combineCols(c("stat", "statistics","statistical"), dtm,"statistics")
dtm <- combineCols(c("model","models","modeling"), dtm,"models")
dtm <- combineCols(c("database","databases"), dtm,"database")
dtm <- combineCols(c("markov_chains","markov_chain"), dtm,"markov_chains")
dtm <- combineCols(c("computational","computationally"), dtm,"computational")
```


```{r ldatuning1 total, cache=TRUE, warning=FALSE}
set.seed(87460945)

# Calculate folds
idxs <- sample(seq_len(9))
folds <- split(idxs, rep(1:3, each = 3, length.out = 9))

# Define number of topics
topics <- seq(2, 50, 1)

# Create data frame for storing results
results <- data.frame()

# Perform cross validation
for (k in topics) {
  scores <- c()
  for (i in 1:3) {
    test_idx <- folds[[i]]
    train_idx <- setdiff(unlist(folds, use.names = FALSE), test_idx)

    test <- dtm[test_idx, ]
    train <- dtm[train_idx, ]

    LDA.out <- LDA(dtm, k, method = "Gibbs")
    p <- perplexity(LDA.out, newdata = test)
    scores <- c(scores, p)
  }
  temp <- data.frame("K" = k, "Perplexity" = mean(scores))
  results <- rbind(results, temp)
}

# Plot Perplexity vs. K
plot <- ggplot(results, aes(x=K, y=Perplexity)) + geom_line() + ggtitle("Perplexity vs. Number of Topics K")+
  theme(plot.title = element_text(hjust = 0.5))


ggsave("./figures/number_of_topics_plot.png", plot)

plot
```

```{r min Perplexity, cache=TRUE}
results[which.min(results$Perplexity),"K"]
```


```{r ldatuning2 total, cache=TRUE, warning=FALSE}
result <- FindTopicsNumber(
  dtm,
  topics = seq(from = 20, to=35, by = 1),
  metrics = c("Griffiths2004", "CaoJuan2009", "Arun2010", "Deveaud2014"),
  method = "Gibbs",
  control = list(seed = 77),
  mc.cores = 15L
)

plot <- FindTopicsNumber_plot(result)

ggsave("./figures/number_of_topics_plot.png", plot)

plot
```



# LDAvis

## 3 Topic Solution

```{r LDAvis Total 3, cache=TRUE}
# Fit largest model
lda.out.3 <-
  LDA(dtm, 3, method = "Gibbs", control = list(seed = 87460945,burnin=5000))

# Find required quantities
phi <- posterior(lda.out.3)$terms %>% as.matrix
theta <- posterior(lda.out.3)$topics %>% as.matrix
vocab <- colnames(phi)
doc_length <- vector()
for (i in 1:length(corpus)) {
  temp <- paste(corpus[[i]]$content, collapse = ' ')
  doc_length <-
    c(doc_length, stri_count(temp, regex = '\\S+'))
}
temp_frequency <- as.matrix(dtm)
freq_matrix <- data.frame(ST = colnames(temp_frequency),
                          Freq = colSums(temp_frequency))

# Convert to json
json_lda <- LDAvis::createJSON(
  phi = phi,
  theta = theta,
  vocab = vocab,
  doc.length = doc_length,
  term.frequency = freq_matrix$Freq
)

# Open server for visualization
# serVis(json_lda, out.dir = "./totalk3", open.browser = FALSE)
serVis(json_lda, open.browser = TRUE)
```

- Topic 1: Statistics
- Topic 2: Programming
- Topic 3: Mathematics

## 9 Topic Solution

```{r LDAvis Total 9, cache=TRUE}
# Seeds that aren't bad so far:
lda.out.9 <-
  LDA(dtm,
      9,
      method = "Gibbs",
      control = list(
        seed = 53896
      ))

# Find required quantities
phi <- posterior(lda.out.9)$terms %>% as.matrix
theta <- posterior(lda.out.9)$topics %>% as.matrix
vocab <- colnames(phi)
doc_length <- vector()
for (i in 1:length(corpus)) {
  temp <- paste(corpus[[i]]$content, collapse = ' ')
  doc_length <-
    c(doc_length, stri_count(temp, regex = '\\S+'))
}
temp_frequency <- as.matrix(dtm)
freq_matrix <- data.frame(ST = colnames(temp_frequency),
                          Freq = colSums(temp_frequency))

# Convert to json
json_lda <- LDAvis::createJSON(
  phi = phi,
  theta = theta,
  vocab = vocab,
  doc.length = doc_length,
  term.frequency = freq_matrix$Freq
)

# Open server for visualization
# serVis(json_lda, out.dir = "./totalk9", open.browser = T)
serVis(json_lda, open.browser = TRUE)
```
- Topic 1: Communication/Business
- Topic 2: Machine Learning
- Topic 3: Programming
- Topic 4: Inference/Data Analysis
- Topic 5: Probability Theory
- Topic 6: Databases
- Topic 7: Mathematics (Optimization?)
- Topic 8: Algorithms and Data Structures
- Topic 9: Calculus

```{r echo=FALSE, results='asis'}
library(htmltools)

# Path to the HTML file
html_file <- "./totalk9/index.html"

# Read and print the HTML file content
cat(paste0(readLines(html_file), collapse = "\n"))
```

- Topic 1: Calculus
- Topic 2: Data Analysis
- Topic 3: Communication
- Topic 4: Algorithms and Data Structures
- Topic 5: Programming
- Topic 6: Probability Theory
- Topic 7: Databases
- Topic 8: Linear Algebra
- Topic 9: Machine Learning

# University-Topic Composition

## 3 Topic SOlution

Double check which topic # is which topic

```{r}
# Tidy the LDA model output for terms
topic_terms <- tidy(lda.out.3, matrix = "beta")

# Get the top 10 terms for each topic
top_terms <- topic_terms %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)


# Optional: Visualize the top terms
top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free_y") +
  coord_flip() +
  scale_x_reordered() +
  labs(title = "Top terms in each LDA topic",
       x = "Term",
       y = "Beta")

```

- Topic 1: programming
- Topic 2: statistics
- Topic 3: mathematics

```{r Graph Gamma University Level, cache=TRUE}
uni_topics <- tidy(lda.out.3,matrix="gamma")


universities <- c("Berkeley","Concordia","Laurier","Manitoba","SFU","Toronto","Waterloo","Western","UBCO")

vizdf <- data.frame(document = NA, topic = NA, gamma = NA)

for (uni in universities) {
  regex <- paste0("^", uni)
  
  test <- subset(uni_topics, topic == 1)
  idx <- which(str_detect(test$document, regex))
  gamma1 <- sum(test[idx, ]$gamma)
  
  test <- subset(uni_topics, topic == 2)
  idx <- which(str_detect(test$document, regex))
  gamma2 <- sum(test[idx, ]$gamma)
  
  test <- subset(uni_topics, topic == 3)
  idx <- which(str_detect(test$document, regex))
  gamma3 <- sum(test[idx, ]$gamma)

  gamma <-  c(gamma1,gamma2,gamma3)
  gamma <- gamma/sum(gamma)


  temp <- data.frame(document = rep(uni,3), topic = c("Programming","Statistics","Mathematics"), gamma = gamma)
  
  vizdf <- rbind(vizdf,temp)
  }

vizdf <- na.omit(vizdf)
vizdf$document[vizdf$document=="Berkeley"] <- "UC Berkeley" 

top_terms <- vizdf %>%
 group_by(topic) %>%
 arrange(topic, gamma)

top_terms %>%
 mutate(topic = reorder(topic, gamma)) %>%
 ggplot(aes(topic, gamma, fill = factor(document))) +
 geom_col(show.legend = FALSE) +
 facet_wrap(~ document, scales = "free_y") +
 coord_flip()
```

## 9 Topic Solution

```{r}
topic_terms <- tidy(lda.out.9, matrix = "beta")

# Get the top 10 terms for each topic
top_terms <- topic_terms %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free_y") +
  coord_flip() +
  scale_x_reordered() +
  labs(title = "Top terms in each LDA topic", x = "Term", y = "Beta")
```

- Topic 1: Machine Learning
- Topic 2: Inference/Data Analysis
- Topic 3: Calculus
- Topic 4: Databases
- Topic 5: Communication/Business
- Topic 6: Algorithms/Data Structures (A&DS)
- Topic 7: Mathematics (Optimization? Applied math?)
- Topic 8: Programming
- Topic 9: Probability Theory

```{r}
uni_topics <- tidy(lda.out.9,matrix="gamma")

universities <- c("Berkeley","Concordia","Laurier","Manitoba","SFU","Toronto","Waterloo","Western","UBCO")

vizdf <- data.frame(document = NA, topic = NA, gamma = NA)

topics <- c("Machine Learning",
            "Data Analysis",
            "Calculus",
            "Databases",
            "Communication",
            "Algorithms/Data Structures",
            "Applied Mathmatics",
            "Programming",
            "Probability Theory")
for (uni in universities) {
  regex <- paste0("^", uni)
  
  gamma <- c()
  for (i in 1:length(topics)) {
  test <- subset(uni_topics, topic == i)
  idx <- which(str_detect(test$document, regex))
  gamma <- c(sum(test[idx, ]$gamma), gamma) 
    
  }

  gamma <- gamma/sum(gamma)

  temp <- data.frame(document = rep(uni,9), topic = topics, gamma = gamma)
  
  vizdf <- rbind(vizdf,temp)
  }

vizdf <- na.omit(vizdf)

top_terms <- vizdf %>%
 group_by(topic) %>%
 arrange(topic, gamma)

top_terms %>%
  mutate(topic = reorder(topic, gamma)) %>%
  ggplot(aes(topic, gamma, fill = factor(document))) +
  theme(axis.text = element_text(size = 6)) +
  geom_col(show.legend = FALSE) +
  facet_wrap( ~ document, scales = "free_y") +
  coord_flip()
```



# Word Clouds

```{r}
uni_topics <- tidy(lda.out.3,matrix="beta")

top_terms <- uni_topics %>%
 group_by(topic) %>%
 top_n(30, beta) %>%
 ungroup() %>%
 arrange(topic, -beta) %>%
  mutate(term = reorder(term, beta))
```

::: panel-tabset
## Topic 1

```{r, echo=FALSE, cache=TRUE}
df <- subset(as.data.frame(top_terms),topic==1)
wordcloud(
  df$term,
  freq = df$beta,
  min.freq = 0.005,
  random.order = FALSE,
  rot.per = 0.35,
  scale=c(2.5,1),
  colors = brewer.pal(8, "Dark2")
)
# title(paste("Topic ",1), font.main = 1)
```

## Topic 2

```{r, echo=FALSE, cache=TRUE}
df <- subset(as.data.frame(top_terms),topic==2)
wordcloud(
  df$term,
  freq = df$beta,
  min.freq = 0.005,
  random.order = FALSE,
  rot.per = 0.35,
  scale=c(2.5,0.8),
  colors = brewer.pal(8, "Dark2")
)
# title(paste("Topic ",2), font.main = 1)
```

## Topic 3

```{r, echo=FALSE, cache=TRUE}
df <- subset(as.data.frame(top_terms),topic==3)
wordcloud(
  df$term,
  freq = df$beta,
  min.freq = 0.005,
  random.order = FALSE,
  rot.per = 0.35,
  scale=c(2.5,1),
  colors = brewer.pal(8, "Dark2")
)
# title(paste("Topic ",3), font.main = 1)

```
:::














