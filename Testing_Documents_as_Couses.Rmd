---
title: "By Course ID Test"
author: "Daniel Krasnov"
date: "2023-04-03"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE, echo=TRUE}
knitr::opts_chunk$set(echo = TRUE)
```

Having the university as the document and the concatenated course descriptions as the text tends to lead to noisy clusters. Therefore we opt to try training our model on a corpus where documents are the courses and their descriptions are the text. Our document number will then go from 9 to approximately 200.

```{r Libraries, echo=FALSE, results='hide'}
# Load Libraries
library(quanteda.textstats)
library(quanteda)
library(tm)
library(topicmodels)
library(ggplot2)
library(ldatuning)
library(LDAvis)
library(dplyr)
library(stringi)
library(stringr)
library(tidytext)
library(tidyr)
library(wordcloud)
library(graphics)
library(slam)
library(gridExtra)
```

We begin by constructing our corpus object,

```{r Load in dataframe}
data <- load("./data/RObjects/degree_corpus_by_course.RData")
colnames(degree_corpus_by_course) <- c("doc_id", "text")
degree_corpus_by_course <- degree_corpus_by_course[!duplicated(degree_corpus_by_course$doc_id),]
degree_corpus_by_course <- degree_corpus_by_course[degree_corpus_by_course$text != "",]
head(degree_corpus_by_course)
```

```{r Construct Corpus}
ds <- DataframeSource(degree_corpus_by_course)
corpus <- Corpus(ds)
inspect(corpus[1])
```

We now clean our corpus. As previously explored using TF-IDF for term inclusion and stemming both lead to more difficult to interpret topics. Therefore we opt very basic cleaning. We also remove custom stopwords found in the previous attempts at modeling this dataset to be cumbersome for interpretation.

```{r}
# Remove punctuation
corpus <- tm_map(corpus, content_transformer(removePunctuation))
# Send everything to lower case
corpus <- tm_map(corpus, content_transformer(tolower))
# Remove stopwords
corpus <- tm_map(corpus,
                 content_transformer(removeWords),
                 stopwords("english"))
# Remove numbers
corpus <- tm_map(corpus, content_transformer(removeNumbers))

# Remove custom stop words
remove_words <-
  c("include","methods","topics","design","functions","emphasis","language","introduction","languages","performance","experience","course","techniques","variables","number","department","tools","fundamental","also","major","modern","issues","used","methods","using","case","architecture","covered","credit","basic","cosc","granted","use","solutions","will","students","fall","spring","important","one","considered","stacks","offers","types","may","held","former","honours","faculty","related","enter","review","enrolment","exercises","summer","need","offered","social","digital","terms","real","concepts","understanding","can","including","programs","program","recommended","examples","introduced","large","search","relations","key","etc","reasoning","intended","fws","general","restricted","version","two","comp","well","rich","intended","required","internet","recent","phys","sciences","covers","year","selected","renewal","explored","csch","principles","practice","development","studies","security","provides","advanced","instruction","discussed","processes","death","lower","high","crncr","taken","efficient","includes","core","retrieval","class","within","present","option","interested","together","session","week","new","order","tables","small","suitable","wide","without","good","introduces","assignments","current","thinking","completed","basics","essential","gain","effective","file","three","many","classes","extensive","tasks","work","meaningful","first","creating","elementary","image"
  )
corpus <- tm_map(corpus, function(x) {
  removeWords(x, remove_words)
})

# Remove whitespace
corpus <- tm_map(corpus, stripWhitespace)

inspect(corpus[1])
```

Through experimentation it has been found that certain unigrams are better interpretable if we construct the most likely bigram the unigram is likely to participate in. This is found through collocations: terms that have a higher probability of appearing together than they do individually. High values of count, $\lambda$ and $z$ all point towards potentially good bigrams to include. We decide term inclusion ultimately by testing different models.

```{r}
# Inspecting BiGrams table

corpusq <- quanteda::corpus(corpus)

corpuz_tokzd <- quanteda::tokens(corpusq)

BiGrams <- corpuz_tokzd %>% 
       quanteda::tokens_select(pattern = "^[A-Z]", 
                               valuetype = "regex",
                               case_insensitive = TRUE, 
                               padding = TRUE) %>% 
       textstat_collocations(min_count = 8, tolower = FALSE,size=2)

BiGrams
```

```{r}
# Plotting BiGrams
plot1 <- ggplot(BiGrams, aes(fill=count, y=lambda, x=reorder(collocation, -lambda))) + 
    geom_bar(position="dodge", stat="identity") + coord_flip()+labs(x= "Bigrams", y = "lambda")

plot2 <- ggplot(BiGrams, aes(fill=count, y=z, x=reorder(collocation, -z))) + 
    geom_bar(position="dodge", stat="identity") + coord_flip()+labs(x= "Bigrams", y = "z")
  
grid.arrange(plot1, plot2, ncol=2)
```

We will need a couple custom functions for combing terms and making bigrams,

```{r Define combineCols}
combineCols <- function(words, dtm, newColName){
  # Get indices of columns
idx <- which(colnames(dtm) %in% words)

# Convert sparse matrix dtm to normal matrix
dtm <- as.matrix(dtm)

# Combine columns
dtm[,c(idx[1])] <- rowSums(dtm[,c(idx)])

# Rename term
colnames(dtm)[idx[1]] <- newColName

if(length(words) == 2){
dtm <- dtm[,-idx[2]]  
}else{
dtm <- dtm[,-idx[2:length(words)]]  
}
# Convert back to DTM
dtm <- as.DocumentTermMatrix(as.simple_triplet_matrix(dtm),weighting = weightTf)

return(dtm)
}
```

```{r Define createBigram}
createBigram <- function(df,term1,term2,replacement){
  regex <- paste0(term1,"\\s+",term2)
  for (i in 1:nrow(df)) {
    row_text <- df[i, 2]
    df[i, 2] <- str_replace(row_text, regex, replacement)
  }
  return(df)
}
```

Now can add bigrams to our corpus,

```{r Create Bigrams}
# Recreate data frame
cor <- as.list(corpus)
df <- cbind(data.frame("doc_id" = names(cor)),data.frame("text"=t(as.data.frame(cor))))

df <- createBigram(df,"data","science","data_science")
df <- createBigram(df,"data","structures","data_structures")
df <- createBigram(df,"data","analysis","data_analysis")
df <- createBigram(df,"data","mining","data_mining")
df <- createBigram(df,"machine","learning","machine_learning")
df <- createBigram(df,"computer","science","computer_science")
df <- createBigram(df,"time","series","time_series")
df <- createBigram(df,"database","systems","database_systems")
df <- createBigram(df,"data","visualization","data_visualization")
df <- createBigram(df,"neural","networks","neural_networks")
df <- createBigram(df,"graph","theory","graph_theory")
df <- createBigram(df,"differential","equations","differential_equations")
df <- createBigram(df,"big","data","big_data")
df <- createBigram(df,"hypothesis","testing","hypothesis_testing")
df <- createBigram(df,"linear","regression","linear_regression")
df <- createBigram(df,"regression","models","regression_models")
df <- createBigram(df,"data","sets","data_sets")
df <- createBigram(df,"text","data","text_data")
```


```{r Recreate Corpus}
ds2 <- DataframeSource(df)
corpus2 <- Corpus(ds2)

# Remove custom stop words
remove_words <-
  c("data","science")
corpus2 <- tm_map(corpus2, function(x) {
  removeWords(x, remove_words)
})

dtm <- DocumentTermMatrix(corpus2)
inspect(dtm)
```

We also manually combine synonymous terms,

```{r}
dtm <-
  combineCols(c("math", "mathematics", "mathematically", "mathematical"),
              dtm,"mathematics")
dtm <- combineCols(c("stat", "statistics","statistical"), dtm,"statistics")
dtm <- combineCols(c("model","models","modeling"), dtm,"models")
dtm <- combineCols(c("database","databases"), dtm,"database")
```

Now we can move onto discovering optimal $K$ values.

```{r ldatuning, cache=TRUE}
# Find number of topics
result <- FindTopicsNumber(
  dtm,
  topics = seq(from = 2, to = 20, by = 1),
  metrics = c("Griffiths2004", "CaoJuan2009", "Arun2010", "Deveaud2014"),
  method = "Gibbs",
  control = list(seed = 77),
  mc.cores = 2L,
)
# Plot metrics
suppressWarnings(FindTopicsNumber_plot(result))
```

```{r Perplexity 3-fold CV, cache=TRUE}
set.seed(87460945)

# Calculate folds
idxs <- sample(seq_len(9))
folds <- split(idxs, rep(1:3, each = 3, length.out = 9))

# Define number of topics
topics <- seq(2, 50, 1)

# Create data frame for storing results
results <- data.frame()

# Perform cross validation
for (k in topics) {
  scores <- c()
  for (i in 1:3) {
    test_idx <- folds[[i]]
    train_idx <- setdiff(unlist(folds, use.names = FALSE), test_idx)

    test <- dtm[test_idx, ]
    train <- dtm[train_idx, ]

    LDA.out <- LDA(dtm, k, method = "Gibbs")
    p <- perplexity(LDA.out, newdata = test)
    scores <- c(scores, p)
  }
  temp <- data.frame("K" = k, "Perplexity" = mean(scores))
  results <- rbind(results, temp)
}

# Plot Perplexity vs. K
ggplot(results, aes(x=K, y=Perplexity)) + geom_line() + ggtitle("Perplexity vs. Number of Topics K")+
  theme(plot.title = element_text(hjust = 0.5))
```

```{r min Perplexity}
# Retrieving K for minimum Perplexity
results[which.min(results$Perplexity),"K"]
```

Some optimal topic numbers are 3, 5, 8, 9, 12, 14, 15, 20, 25. Now we can visualize them with LDAvis noting that we set $\lambda=0.6$.

```{r LDAvvis 3 Group Sol}
# Fit largest model
lda.out <- LDA(dtm, 3, method = "Gibbs",control = list(seed = 87460945))

# Find required quantities
phi <- posterior(lda.out)$terms %>% as.matrix
theta <- posterior(lda.out)$topics %>% as.matrix
vocab <- colnames(phi)
doc_length <- vector()
for (i in 1:length(corpus)) {
  temp <- paste(corpus[[i]]$content, collapse = ' ')
  doc_length <-
    c(doc_length, stri_count(temp, regex = '\\S+'))
}
temp_frequency <- as.matrix(dtm)
freq_matrix <- data.frame(ST = colnames(temp_frequency),
                          Freq = colSums(temp_frequency))

# Convert to json
json_lda <- LDAvis::createJSON(
  phi = phi,
  theta = theta,
  vocab = vocab,
  doc.length = doc_length,
  term.frequency = freq_matrix$Freq
)

# Open server for visualization
# serVis(json_lda, out.dir = "./TestVis/K_3", open.browser = FALSE)
# serVis(json_lda, open.browser = TRUE)
```

View the visualization [here](https://danyulll.github.io/vis_k_3/).

As we might expect the overarching topics of any Data Science curriculum are programming, statistics, and mathematics. Importantly, models actually beats out statistics indicating that data scientists are primarily concerned with modelling as opposed to inference or estimation.

```{r LDAvvis 5 Group Sol}
# Fit largest model
lda.out <- LDA(dtm, 5, method = "Gibbs",control = list(seed = 87460945))

# Find required quantities
phi <- posterior(lda.out)$terms %>% as.matrix
theta <- posterior(lda.out)$topics %>% as.matrix
vocab <- colnames(phi)
doc_length <- vector()
for (i in 1:length(corpus)) {
  temp <- paste(corpus[[i]]$content, collapse = ' ')
  doc_length <-
    c(doc_length, stri_count(temp, regex = '\\S+'))
}
temp_frequency <- as.matrix(dtm)
freq_matrix <- data.frame(ST = colnames(temp_frequency),
                          Freq = colSums(temp_frequency))

# Convert to json
json_lda <- LDAvis::createJSON(
  phi = phi,
  theta = theta,
  vocab = vocab,
  doc.length = doc_length,
  term.frequency = freq_matrix$Freq
)

# Open server for visualization
# serVis(json_lda, out.dir = "./TestVis/K_5", open.browser = FALSE)
# serVis(json_lda, open.browser = TRUE)
```

View the visualization [here](https://danyulll.github.io/vis_k_5/).

Now topics begin to become more nuanced:
* Topic 1: statistics comes to the top with applications, random, theory, probability, estimation, and distributions trailing behind. Clearly a statistics topic.
* Topic 2: software, database, and systems come to the top of this topic. Databases are an important part of any data scientist's job and it makes sense this would be put into its own topic- its quite distinct from any of the others.
* Topic 3: models, analysis, learning, regression, clustering, classification, and machine_learning. Very clearly a machine learning topic.
* Topic 4: mathematics is by in large the most relevant term. This coupled with words such as calculus, equations, linear, integration shows this is a mathematics topic.
* Topic 5: programming, algorithms, computer, objectorientated, this is likely the computer science and programming aspect of any data science program.

```{r LDAvvis 8 Group Sol}
# Fit largest model
lda.out <- LDA(dtm, 8, method = "Gibbs",control = list(seed = 87460945))

# Find required quantities
phi <- posterior(lda.out)$terms %>% as.matrix
theta <- posterior(lda.out)$topics %>% as.matrix
vocab <- colnames(phi)
doc_length <- vector()
for (i in 1:length(corpus)) {
  temp <- paste(corpus[[i]]$content, collapse = ' ')
  doc_length <-
    c(doc_length, stri_count(temp, regex = '\\S+'))
}
temp_frequency <- as.matrix(dtm)
freq_matrix <- data.frame(ST = colnames(temp_frequency),
                          Freq = colSums(temp_frequency))

# Convert to json
json_lda <- LDAvis::createJSON(
  phi = phi,
  theta = theta,
  vocab = vocab,
  doc.length = doc_length,
  term.frequency = freq_matrix$Freq
)

# Open server for visualization
# serVis(json_lda, out.dir = "./TestVis/K_8", open.browser = FALSE)
# serVis(json_lda, open.browser = TRUE)
```
View the visualization [here](https://danyulll.github.io/vis_k_8/).

For the sake of brevity I will simply list what I consider to be the most likely topics from here on out. Note asterisks indicate noisey topics I am unsure of.

* Topic 1: statistics
* Topic 2: machine learning
* Topic 3: business
* Topic 4: mathematics
* Topic 5: data analysis/inference
* Topic 6: databases
* Topic 7: computer science
* Topic 8: linear algebra


```{r LDAvvis 9 Group Sol}
# Fit largest model
lda.out <- LDA(dtm, 9, method = "Gibbs",control = list(seed = 87460945))

# Find required quantities
phi <- posterior(lda.out)$terms %>% as.matrix
theta <- posterior(lda.out)$topics %>% as.matrix
vocab <- colnames(phi)
doc_length <- vector()
for (i in 1:length(corpus)) {
  temp <- paste(corpus[[i]]$content, collapse = ' ')
  doc_length <-
    c(doc_length, stri_count(temp, regex = '\\S+'))
}
temp_frequency <- as.matrix(dtm)
freq_matrix <- data.frame(ST = colnames(temp_frequency),
                          Freq = colSums(temp_frequency))

# Convert to json
json_lda <- LDAvis::createJSON(
  phi = phi,
  theta = theta,
  vocab = vocab,
  doc.length = doc_length,
  term.frequency = freq_matrix$Freq
)

# Open server for visualization
# serVis(json_lda, out.dir = "./TestVis/K_9", open.browser = FALSE)
# serVis(json_lda, open.browser = TRUE)
```

View the visualization [here](https://danyulll.github.io/vis_k_9/).

* Topic 1: probability theory
* Topic 2: machine learning
* Topic 3: business analytics/marketing
* Topic 4: mathematics
* Topic 5: databases
* Topic 6: data analysis*
* Topic 7: computer science
* Topic 8: business applications*
* Topic 9: communication


```{r LDAvvis 12 Group Sol}
# Fit largest model
lda.out <- LDA(dtm, 12, method = "Gibbs",control = list(seed = 87460945))

# Find required quantities
phi <- posterior(lda.out)$terms %>% as.matrix
theta <- posterior(lda.out)$topics %>% as.matrix
vocab <- colnames(phi)
doc_length <- vector()
for (i in 1:length(corpus)) {
  temp <- paste(corpus[[i]]$content, collapse = ' ')
  doc_length <-
    c(doc_length, stri_count(temp, regex = '\\S+'))
}
temp_frequency <- as.matrix(dtm)
freq_matrix <- data.frame(ST = colnames(temp_frequency),
                          Freq = colSums(temp_frequency))

# Convert to json
json_lda <- LDAvis::createJSON(
  phi = phi,
  theta = theta,
  vocab = vocab,
  doc.length = doc_length,
  term.frequency = freq_matrix$Freq
)

# Open server for visualization
# serVis(json_lda, out.dir = "./TestVis/K_12", open.browser = FALSE)
# serVis(json_lda, open.browser = TRUE)
```

View the visualization [here](https://danyulll.github.io/vis_k_12/).

* Topic 1: machine learning
* Topic 2: probability theory
* Topic 3: statistics/inference
* Topic 4: communication
* Topic 5: mathematics
* Topic 6: linear algebra
* Topic 7: databases
* Topic 8: algorithms, data structures, computer science
* Topic 9: programming
* Topic 10: applications*
* Topic 11: data analysis*
* Topic 12: proofs

```{r LDAvvis 14 Group Sol}
# Fit largest model
lda.out <- LDA(dtm, 14, method = "Gibbs",control = list(seed = 87460945))

# Find required quantities
phi <- posterior(lda.out)$terms %>% as.matrix
theta <- posterior(lda.out)$topics %>% as.matrix
vocab <- colnames(phi)
doc_length <- vector()
for (i in 1:length(corpus)) {
  temp <- paste(corpus[[i]]$content, collapse = ' ')
  doc_length <-
    c(doc_length, stri_count(temp, regex = '\\S+'))
}
temp_frequency <- as.matrix(dtm)
freq_matrix <- data.frame(ST = colnames(temp_frequency),
                          Freq = colSums(temp_frequency))

# Convert to json
json_lda <- LDAvis::createJSON(
  phi = phi,
  theta = theta,
  vocab = vocab,
  doc.length = doc_length,
  term.frequency = freq_matrix$Freq
)

# Open server for visualization
# serVis(json_lda, out.dir = "./TestVis/K_14", open.browser = FALSE)
# serVis(json_lda, open.browser = TRUE)
```

View the visualization [here](https://danyulll.github.io/vis_k_14/).

* Topic 1: machine learning
* Topic 2: statistics
* Topic 3: communication
* Topic 4: mathematics
* Topic 5: databases
* Topic 6: linear algebra
* Topic 7: research*
* Topic 8: data analysis
* Topic 9: ?
* Topic 10: programming
* Topic 11: data_structures
* Topic 12: inference
* Topic 13: programming*
* Topic 14: applications*

```{r LDAvvis 15 Group Sol}
# Fit largest model
lda.out <- LDA(dtm, 15, method = "Gibbs",control = list(seed = 87460945))

# Find required quantities
phi <- posterior(lda.out)$terms %>% as.matrix
theta <- posterior(lda.out)$topics %>% as.matrix
vocab <- colnames(phi)
doc_length <- vector()
for (i in 1:length(corpus)) {
  temp <- paste(corpus[[i]]$content, collapse = ' ')
  doc_length <-
    c(doc_length, stri_count(temp, regex = '\\S+'))
}
temp_frequency <- as.matrix(dtm)
freq_matrix <- data.frame(ST = colnames(temp_frequency),
                          Freq = colSums(temp_frequency))

# Convert to json
json_lda <- LDAvis::createJSON(
  phi = phi,
  theta = theta,
  vocab = vocab,
  doc.length = doc_length,
  term.frequency = freq_matrix$Freq
)

# Open server for visualization
# serVis(json_lda, out.dir = "./TestVis/K_15", open.browser = FALSE)
# serVis(json_lda, open.browser = TRUE)
```

View the visualization [here](https://danyulll.github.io/vis_k_15/).

* Topic 1: probability theory
* Topic 2: machine learning
* Topic 3: business
* Topic 4: communication
* Topic 5: statistics/inference
* Topic 6: mathematics
* Topic 7: calculus
* Topic 8: databases
* Topic 9: programming
* Topic 10: algorithms/data structures
* Topic 11: linear regression or linear algebra
* Topic 12: modelling
* Topic 13: statistics*
* Topic 14: experiment design
* Topic 15: analytics*

```{r LDAvvis 20 Group Sol}
# Fit largest model
lda.out <- LDA(dtm, 20, method = "Gibbs",control = list(seed = 87460945))

# Find required quantities
phi <- posterior(lda.out)$terms %>% as.matrix
theta <- posterior(lda.out)$topics %>% as.matrix
vocab <- colnames(phi)
doc_length <- vector()
for (i in 1:length(corpus)) {
  temp <- paste(corpus[[i]]$content, collapse = ' ')
  doc_length <-
    c(doc_length, stri_count(temp, regex = '\\S+'))
}
temp_frequency <- as.matrix(dtm)
freq_matrix <- data.frame(ST = colnames(temp_frequency),
                          Freq = colSums(temp_frequency))

# Convert to json
json_lda <- LDAvis::createJSON(
  phi = phi,
  theta = theta,
  vocab = vocab,
  doc.length = doc_length,
  term.frequency = freq_matrix$Freq
)

# Open server for visualization
# serVis(json_lda, out.dir = "./TestVis/K_20", open.browser = FALSE)
# serVis(json_lda, open.browser = TRUE)
```

View the visualization [here](https://danyulll.github.io/vis_k_20/).

* Topic 1: probability theory
* Topic 2: machine learning
* Topic 3: data mining
* Topic 4: mathematics
* Topic 5: databases
* Topic 6: calculus
* Topic 7: statistics/inference
* Topic 8: algorithms/data structures
* Topic 9: communication
* Topic 10: projects*
* Topic 11: data analysis/ hypothesis testing
* Topic 12: linear algebra
* Topic 13: experiment design
* Topic 14: programming
* Topic 15: ?
* Topic 16: ?
* Topic 17: object orientated programming*
* Topic 18: regression
* Topic 19: ?
* Topic 20: statistics

```{r LDAvvis 25 Group Sol}
# Fit largest model
lda.out <- LDA(dtm, 25, method = "Gibbs",control = list(seed = 87460945))

# Find required quantities
phi <- posterior(lda.out)$terms %>% as.matrix
theta <- posterior(lda.out)$topics %>% as.matrix
vocab <- colnames(phi)
doc_length <- vector()
for (i in 1:length(corpus)) {
  temp <- paste(corpus[[i]]$content, collapse = ' ')
  doc_length <-
    c(doc_length, stri_count(temp, regex = '\\S+'))
}
temp_frequency <- as.matrix(dtm)
freq_matrix <- data.frame(ST = colnames(temp_frequency),
                          Freq = colSums(temp_frequency))

# Convert to json
json_lda <- LDAvis::createJSON(
  phi = phi,
  theta = theta,
  vocab = vocab,
  doc.length = doc_length,
  term.frequency = freq_matrix$Freq
)

# Open server for visualization
# serVis(json_lda, out.dir = "./TestVis/K_25", open.browser = FALSE)
# serVis(json_lda, open.browser = TRUE)
```

View the visualization [here](https://danyulll.github.io/vis_k_25/).

* Topic 1: probability theory
* Topic 2: calculus
* Topic 3: modelling*
* Topic 4: machine learning
* Topic 5: linear algebra
* Topic 6: databases
* Topic 7: statistics
* Topic 8: algorithms
* Topic 9: mathematics
* Topic 10: programming
* Topic 11: inference
* Topic 12: analytics*
* Topic 13: experiment design
* Topic 14: writing*
* Topic 15: projects*
* Topic 16: business
* Topic 17: computational methods*
* Topic 18: text data*
* Topic 19: data mining
* Topic 20: ?
* Topic 21: data visualization*
* Topic 22: regression
* Topic 23: time series
* Topic 24: data analysis*
* Topic 25: research/marketing



