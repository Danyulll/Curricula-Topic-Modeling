---
title: "Curricula-Topic-Modeling"

toc: true
toc-depth: 3
---

## Introduction to Topic Modeling and LDA

### Terminology

Before beginning there are three important preliminary definitions one will need in any text analysis:

- Document: a distinct text object that one wishes to analyze. This could be a paper, a paragraph, etc.
- Term: an individual word in a given document.
- Corpus: The set of all documents.

### Topic Models

Topic Modeling is a statistical model that attempts to cluster the words found in a document into various "topics". The hope is that these topics would capture some underlying subject contained within the text. It is important to note that Topic Models are fuzzy in the sense that documents are assumed to be comprised of multiple topics with varying degrees of membership to each. 

### Latent Dirichlet Allocation

Perhaps the most famous topic model is *Latent Dirichlet Allocation* (LDA) (citation to LDA paper). It is a three-level hierarchical Bayesian model that defines a collection of documents as a random mixture over some underlying topic set where each topic is itself a distribution over our vocabulary.

LDA works by assuming a data generating process (DGP) for our documents and then employs inference to discover the most likely parameters (reference to text as data book).

Blei, Ng, and Jordan define the following DGP:

1. Choose $N \sim$ Poisson($\xi$)
2. Choose $\theta \sim$ Dir($\alpha$)
3. For each of the $N$ words $w_n$:
    1. Choose a topic $z_n\sim$Multinomial($\theta$)
    2. Choose a word $w_n$ from $p(w_n|z_n,\beta)$, a multinomial probability conditioned on the topic $z_n$
  
For the following analysis we estimate the parameters using the *collapsed Gibbs sampling method*, though it is important to note there are others that yield varied results. To further complicate things, when fitting LDA in R one must predefine the number of topics for the model. Finding a good estimate for the number of topics is paramount and many methods are explored.

## Feature Extraction and Preprocessing

### Document-Term Matrices

There are many methods of feature extraction from text, we opt for the *bag of words* model. To construct this model simply define a common set of words shared between documents and store a count of word appearances for each document. Commonly this is stored as a *Document-Term Matrix* (DTM), for example, given the documents:

::: {#tbl-panel layout-ncol=2}
| Document 1 |
|------|
| "I am happy"    |




| Document 2 | 
|------|
| "I am sad"   | 




Corpus
:::


the corresponding DTM would be

||I | am | happy | sad |
|---------|:-----|------:|:------:|
| **Doc 1** |1      | 1   |    1 |   0   |
| **Doc 2**  | 1   | 1  |   0 |  1   |



In R we can us the tm package to create our Corpus and DTM objects. For these examples scrapped Data Science course information, from various universities, will be preprocessed. 

First this data will be examined from a degree pathway perspective with the documents being concatenated course descriptions for entire curricula. These curricula are constructed for each university by sampling courses from each university's Data Science course calendar.

```{r Load in Data}
# Loading in data
load("./data/RObjects/degree_corpus.RData")

# Examining which universities are represented 
print(degree_corpus$doc_id)
```

```{r Constructing Corpus}
library(tm)
ds <- DataframeSource(degree_corpus)
corpus <- Corpus(ds)
inspect(corpus[1])
```
As shown the first element of the corpus is from Western University and contains all course description terms from a given pathway through their Data Science calendar. 

### Complexity Reduction

Raw text data lends itself to difficult analysis. NLP posses several best practices for cleaning text:

- Punctation/Number Removal: deleting any non alphabetic characters.
- Lowercasing: sending all words to lowercase.
- Stopword Removal: stopwords are words used often in sentences that give little to no information, i.e., atricles such as the, a, etc.
- Stemming: truncating the ends of words so that they share a common base, i.e., fishing and fishes would be transformed ot fish.
- Tokenizing: dividing a document into a set of individual words

```{r Text Preprocessing}
# Remove punctuation
corpus <- tm_map(corpus, content_transformer(removePunctuation))
# Send everything to lower case
corpus <- tm_map(corpus, content_transformer(tolower))
# Remove stopwords
corpus <- tm_map(corpus, content_transformer(removeWords), 
          stopwords("english"))
# Remove whitespace
corpus <- tm_map(corpus, stripWhitespace)
# Remove numbers
corpus <- tm_map(corpus, content_transformer(removeNumbers))
# Remove custom stop words
corpus <- tm_map(corpus, function(x) {removeWords(x,c("data","will","students","fall","spring","important","one","considered","stacks","offers","types","may","held","former","honours","faculty","related","enter","review","enrolment","exercises","summer","need","offered"))})
# Stemming
corpus <- tm_map(corpus, stemDocument)

inspect(corpus[1])
```
Now we may construct our DTM,
```{r DTM Construction}
# Create DTM object
dtm <- DocumentTermMatrix(corpus)
inspect(dtm[,1:9])
```

### TF-IDf

If one wishes to further reduce complexity there exist various heuristics for term inclusion. One such heuristic is *term frequency inverse document frequency* (TF-IDF). The intuition for TF-IDF is that extremely frequent words hold language together but do not provide great insight of topic or context. Meanwhile rare words contain a plethora of information but lack the frequency needed to be used in generalizations. Therefore words somewhere in the middle of these exetrems ought to be included (text as data citation).

The quantity itself is the multiplication of the *term frequency* (TF) and *inverse document drequency* (IDF). TF is simply the number of times the term $j$ appears in a given document. IDF is defined as

$$idf(term)=ln\left(\frac{{\text{\# of doucments}}}{{\text{\# documents containing term j}}}\right)$$
IDF is a penalty term that adjusts term frequency based on the term's rarity (text mining in R citation).

```{r TF-IDF weighting}
tfidf_dtm <- weightTfIdf(dtm)
inspect(tfidf_dtm[,1:5])
```
Now our DTM contains TF-IDF scores as the entries as opposed to just TF. We can now filter away terms below a certain threshold of TF-IDF. A common approach is to filter terms away below the median.

```{r TF-IDF Median Calc}
tfidf_mat <- as.matrix(tfidf_dtm)
median(tfidf_mat[tfidf_mat > 0])
```
```{r Filter TF-IDF below median}
tfidf_dtm_r <- tfidf_dtm[,tfidf_dtm$v > 0.0022]
```

```{r TF-IDF DTM Dimensionality}
# Dimensions before reduction
dim(tfidf_dtm)

# Dimensions after reduction
dim(tfidf_dtm_r)
```
With this our dimensionality has sufficiently reduced.

## Exploring Degree Pathways

### Topic Number Discovery
LDA, like many unsupervised learning methods, is extremely dependent on its parameters. One such parameter that requires meticulous tuning is the number of topics. In the literature there exist many metrics for discovering the optimal number of topics with several being readily available in R.

#### Coherence




### Finding the Number of Topics


## Exploring Courses

## References

Need a reference for LDA paper, text as data, text mining with R

https://knowledger.rbind.io/post/topic-modeling-using-r/

https://datamathstat.wordpress.com/2019/10/25/text-preprocessing-for-nlp-and-machine-learning-using-r/

https://ldavis.cpsievert.me/reviews/reviews.html

https://www.r-bloggers.com/2015/05/a-link-between-topicmodels-lda-and-ldavis/

